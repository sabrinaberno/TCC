\chapter[Metodologia]{Metodologia}

\section{Classificação da Pesquisa}

\subsection{Natureza}

Em relação à natureza desta pesquisa, trata-se de uma pesquisa aplicada. Temos como objetivo principal gerar um conhecimento que possa ter um impacto direto e uma utilidade prática, ambos em contextos reais \cite{nascimento2016}. 

Com esta pesquisa, o desenvolvimento do modelo de IA tem a potencialidade de melhorar os sucessos dos tratamentos de FIV, fazendo o processo de seleção embrionária mais eficaz, menos invasivo e mais acessível a um maior número de pessoas.

\subsection{Método ou Abordagem Metodológica}

A metodologia ou abordagem metodológica dessa pesquisa é quantitativa \cite{nascimento2016}. Nosso foco é a análise dos dados numéricos referentes aos padrões morfocinéticos de embriões. Esses dados serão utilizados para o desenvolvimento da IA, que será capaz de prever a porcentagem de euploidia, auxiliando na seleção de embriões com maior possibilidade de saúde genética.

Escolher a abordagem quantitativa nos ajudará a atingir os objetivos desta pesquisa, permitindo explorar e validar os dados com precisão, oferecendo resultados objetivos. 

\subsection{Objetivos}

Quanto aos objetivos, o objetivo desta pesquisa é exploratório \cite{nascimento2016}. Este trabalho procura identificar e investigar padrões em dados morfocinéticos de embriões, usando o TLS, explorando a possibilidade de realizar essa predição juntamente com as tecnologias de IA.

Ao focar na concepção de um modelo que terá a capacidade de identificar padrões nos dados,exploraremos a relação entre esses dados e a importância de cada padrão para o resultado desejado, compreendendo os fatores que influenciam a saúde genética dos embriões, mas sem um conhecimento prévio estabelecido que explique completamente essas relações.

\subsection{Procedimentos De Pesquisa}

O procedimento adotado neste trabalho é experimental. Definimos este procedimento por causa do objetivo de investigar as relações entre as variáveis, o que é uma característica da pesquisa experimental \cite{nascimento2016}. Buscamos estabelecer uma relação de causa e efeito entre as características morfocinéticas dos embriões, mais especificamente a porcentagem de euploidia.

\section{Design da Pesquisa}

Esse estudo adotará o uso de IA para realizar a análise dos dados morfogenéticos dos embriões, desenvolvendo um modelo de predição baseado em machine learning. O modelo será treinado para identificar os padrões nos dados coletados pelo TLS, com foco em prever a porcentagem de euploidia, o que indica a saúde genética dos embriões.

Para desenvolver e testar o modelo, utilizaremos a linguagem de programação Python, aproveitando as bibliotecas disponíveis para a construção da IA. Quanto à validação do modelo, será elaborada uma fase experimental, na qual o modelo será testado com dados reais de embriões já classificados, a fim de compararmos e testarmos seu desempenho, refinando-o quando necessário.

Nesta pesquisa, buscaremos identificar e mapear os padrões em um campo que ainda está em desenvolvimento. A prática será testada em um ambiente controlado com dados obtidos pelo TLS, avaliando a efetividade do modelo com base na sua capacidade de prever, em porcentagem, a ploidia do embrião.

\subsection{Fases de Trabalho}

As fases do nosso trabalho se dividem em duas etapas: \textbf{Fase 1: Análise e Preparação de Dados} (Tabela \ref{tab:fase1}), com o objetivo de compreender e organizar os dados para realizar a análise da influência dos parâmetros na porcentagem de euploidia, e a \textbf{Fase 2: Desenvolvimento e Avaliação do Modelo} detalhada na Tabela (Tabela \ref{tab:fase2}), que foca no desenvolvimento, ajuste e avaliação de um modelo de ML para efetuar a predição de euploidia, finalizando com a entrega de um protótipo de uma interface a ser evoluída em trabalhos futuros, realizando a criação e junção dos dois.

Nas seções a seguir, demonstraremos os objetivos de cada fase, mostrando suas atividades, nas quais estão descritos resumidamente o que será feito, qual método será utilizado e o resultado esperado. Em seguida, detalharemos cada parte.

\begin{table}[h!]
  \centering
  \renewcommand{\arraystretch}{1.0} 
  \captionsetup{font=footnotesize, justification=centering, labelsep=period, position=above}
  \caption{Fase 1: Análise e Preparação de Dados}
  \label{tab:fase1}
  \resizebox{\textwidth}{!}{ 
    \begin{tabular}{|p{3cm}|p{4cm}|p{3cm}|p{5cm}|} 

      \hline
      \multicolumn{4}{|c|}{\cellcolor[HTML]{008940} \textcolor{white}{\textbf{Fase 1: Análise e Preparação de Dados}}} \\
      \hline

      \midrule

      \vspace{0.2cm} \cellcolor[HTML]{C0C0C0} \textbf{Objetivos Específicos} \vspace{0.2cm} & 
      \vspace{0.2cm} \cellcolor[HTML]{C0C0C0} \textbf{Atividades} \vspace{0.2cm} & 
      \vspace{0.2cm} \cellcolor[HTML]{C0C0C0} \textbf{Método de Pesquisa} \vspace{0.2cm} & 
      \vspace{0.2cm} \cellcolor[HTML]{C0C0C0} \textbf{Resultados Esperados}\vspace{0.2cm} \\

      \midrule

      \multirow{4}{*}{\cellcolor[HTML]{133e78} \textcolor{white}{\parbox[c]{\linewidth}{\centering \textbf{OE1 \\
      Expansão, Processamento e Análise de Dados para Predição de Ploidia}}}} &
      \textcolor[HTML]{133E78}{\textbf{Atividade 1 (A1)}} \newline
      Análise, Revisão e Seleção de Variáveis para Predição de Euploidia &
      - Pesquisa bibliográfica \newline

      - Python - Biblioteca Pandas &
      - Analisar, Revisar e Selecionar as variáveis \newline
      - Limpeza dos dados \\

      \cline{2-4}

      \cellcolor[HTML]{133e78} & 
      \textcolor[HTML]{133E78}{\textbf{Atividade 2 (A2)}} \newline
      Normalização dos Dados para Otimização &
      - Z-Score &
      - Normalização das variáveis numéricas \\

      \cline{2-4}

      \cellcolor[HTML]{133e78} & 
      \textcolor[HTML]{133E78}{\textbf{Atividade 3 (A3)}} \newline
      Identificação da Correlação e Atribuição de Pesos aos Parâmetros na Previsão da Ploidia do Embrião &
      - Coeficiente de correlação de Spearman &
      - Análise das correlações entre variáveis pelo Gráfico de dispersão \\

      \cline{2-4}

      \cellcolor[HTML]{133e78} & 
      \vspace{0.2cm} \textcolor[HTML]{133E78}{\textbf{Atividade 4 (A4)}} \newline
      Divisão de Dados e Aplicação de Data Augmentation \vspace{0.2cm} & 
      - Divisão do conjunto de dados \newline
      - Data augmentation com o Algoritmo de Monte Carlo &
      - Dados para treinamento, validação e teste \newline
      - Aumento do conjunto de dados para treinamento \\

      \hline
    \end{tabular}
  }
\end{table} 
\FloatBarrier  %impede que o texto flutue para outra página (garante que a tabela fique nessa ordem)

\subsubsection{\textbf{Objetivo Específico 1} - Identificação de Parâmetros em Embriões}

\paragraph{\textbf{Atividade 1 (A1):} Análise, Revisão, Seleção e Limpeza de Variáveis para Predição de Euploidia}

Começaremos com a verificação da pertinência das variáveis já existentes na planilha de dados dos embriões e com a avaliação da introdução de outras variáveis que possam aprimorar a exatidão da análise, ou até mesmo com a eliminação de variáveis, se necessário. Em seguida, faremos a limpeza dos dados, substituindo valores nulos por valores mais apropriados em alguns casos, como nas colunas que realizam cálculos entre outras colunas, onde será possível identificar valores e substituí-los.

Para realizar a verificação da pertinência das variáveis já existentes e avaliar a viabilidade de introduzir novas variáveis, utilizaremos a pesquisa bibliográfica, nos norteando pelos estudos apresentados no capítulo 2: o de \citeonline{yuan2023}, o artigo 'Development of an artificial intelligence-based model for predicting the euploidy of blastocysts in PGT-A treatments' e o de \citeonline{souzarebeca2022}, 'Análise da ploidia de embriões humanos por meio da inteligência artificial com o uso de variáveis de morfologia, morfocinética e variáveis relacionadas com a paciente'. Ambos os artigos descrevem o uso de IA para fazer a predição da ploidia de embriões, o que se assemelha com o que queremos propor, com a diferença de que temos o objetivo final de prever a porcentagem de aneuploidia. 

Dessa forma, analisaremos os estudos feitos por ambos pesquisadores e utilizaremos para entender o poder que cada variável tem para o objetivo final e se é necessário adicionar outras variáveis que não existem na nossa planilha, ou até mesmo excluí-las. Além disso, conduziremos entrevistas com o especialista encarregado de fornecer os dados, para uma análise prática da pertinência das variáveis disponíveis e para debater possíveis variáveis extras que possam ser relevantes para a análise. As entrevistas possibilitarão alinhar a seleção das variáveis ao conhecimento clínico e experiência prática do profissional, garantindo que as variáveis selecionadas sejam aplicáveis no cenário real de previsão de euploidia. Em relação a limpeza dos dados, utilizaremos a linguagem \textit{Python} que possui a biblioteca \textit{Pandas}, que permite o carregamento de planilhas do Excel, do formato .xlsx, como a que possuímos. De acordo com \citeonline{chen2018}: “O Pandas é uma biblioteca Python de código aberto para análise de dados. Ele dá a Python a capacidade de trabalhar com dados do tipo planilha, permitindo carregar, manipular, alinhar e combinar dados rapidamente, entre outras funções.”.Usaremos as funções \textit{isnull()} e \textit{info()}, da biblioteca \textit{Pandas}.

Inicialmente, analisar as variáveis possibilitando a detecção de variáveis irrelevantes ou redundantes, aprimorando a correlação entre os parâmetros estudados e a percentagem de euploidia. Também a detecção e correção de inconsistências, como valores em branco ou discrepâncias, garantindo que o conjunto de dados esteja organizado e pronto para futuras análises, prevenindo distorções nos resultados. 

\paragraph{\textbf{Atividade 2 (A2):} Normalização dos Dados para Otimização}

As variáveis numéricas são normalizadas, por meio de uma técnica de normalização, o Z-Score, assegurando os intervalos de valores de cada coluna. A normalização é de grande importância que façamos a normalização dos dados, visto que, de acordo com \citeonline{milewski2016} estudos anteriores demonstraram que a incorporação de dados morfológicos normalizados para avaliação da qualidade do embrião aumenta consideravelmente o poder preditivo dos modelos criados. A normalização é uma forma de dimensionar recursos, transformando o intervalo deles em uma escala padrão \cite{jaiswal2024}. Também é importante citar que dados normalizados também são fáceis de interpretar e, portanto, mais fáceis de entender. Quando todos os recursos de um conjunto de dados estão na mesma escala, também se torna mais fácil identificar e visualizar as relações entre diferentes recursos e fazer comparações significativas. \cite{jaiswal2024}. Utilizaremos o método Z-Score, detalhado no APÊNDICE \ref{apendice:zscore}.

Por fim, a normalização das variáveis assegurará que todas estejam dentro do mesmo intervalo, eliminando vieses numéricos e aprimorando a exatidão dos algoritmos de aprendizado de máquina que serão implementados posteriormente.

\paragraph{\textbf{Atividade 3 (A3):} Identificação da Correlação e Atribuição de Pesos aos Parâmetros na Previsão da Ploidia do Embrião}

O objetivo desta atividade é identificar as relações entre os diferentes parâmetros presentes na planilha de dados dos embriões, avaliando a intensidade e o sentido dessas relações, com foco em sua influência na porcentagem de euploidia. Após a pesquisa bibliográfica, utilizaremos o coeficiente de correlação de Spearman, que mede a relação monótona entre duas variáveis, considerando as ordens atribuídas às observações em vez dos valores originais \cite{sousa2019}. A correlação será calculada para todas as combinações possíveis de variáveis, possibilitando uma análise mais detalhada de suas interações \cite{sousa2019}. Um gráfico de dispersão será gerado para complementar a análise visual, facilitando a identificação de padrões. Também com o conhecimento adquirido pela A1, determinaremos a relevância relativa de cada parâmetro na previsão da ploidia do embrião, atribuindo pesos que reflitam sua influência, identificando a relevância de cada parâmetro.

Para realizar a análise de correlação entre os parâmetros será utilizado o coeficiente de Spearman, um método estatístico amplamente empregado para avaliar a intensidade e o sentido da relação monótona entre duas variáveis. Inicialmente, as variáveis do conjunto de dados serão classificadas em ordem crescente, atribuindo-lhes ranks que serão usados para o cálculo do coeficiente. Essa abordagem permite capturar relações tanto lineares quanto não lineares entre as variáveis \cite{sousa2019}, explicado com mais detalhes no APÊNDICE \ref{apendice:spearman}. 

A fórmula do coeficiente de correlação de Spearman será aplicada, utilizando as ordens atribuídas, assegurando que o método se adapte a diferentes formatos de relação, como curvas monótonas crescentes ou decrescentes. Além disso, será realizada uma análise complementar com gráficos de dispersão, que ajudarão a identificar a inclinação dos dados e o sentido da correlação, sendo positiva (próximas ao valor 1) quando as variáveis variam no mesmo sentido e negativa (próximas ao valor -1) quando variam em sentidos opostos \cite{sousa2019}. O resultado numérico do coeficiente será avaliado em relação à sua magnitude, indicando se a correlação é forte, moderada ou fraca, e seu sinal indicará o tipo de relação (positiva ou negativa). 

Usaremos a biblioteca Pandas para manipulação dos dados e a SciPy para calcular o coeficiente de Spearman. A metodologia para a definição e atribuição de pesos específicos aos parâmetros relevantes para a ploidia do embrião combina análise teórica e prática. Inicialmente, será realizada uma pesquisa bibliográfica em publicações científicas e revisões sistemáticas que explorem a influência dos parâmetros na ploidia embrionária.

Os cálculos realizados com o coeficiente de Spearman permitirão identificar quais parâmetros possuem uma relação mais forte (positiva ou negativa) com a porcentagem de euploidia, além de explorar como esses parâmetros se relacionam entre si. Os gráficos gerados deverão destacar as variáveis mais influentes, os padrões visuais que reforcem as relações monótonas, áreas onde a ausência de correlação linear não exclua outras formas de relação. Também resultará em uma lista detalhada dos parâmetros identificados, acompanhada das respectivas justificativas para os pesos atribuídos a cada um, com base em sua influência na ploidia do embrião. Em vez de valores numéricos, os resultados trarão explicações fundamentadas cientificamente, apresentando as razões pelas quais cada parâmetro exerce determinada influência. O resultado final fornecerá uma visão qualitativa e consistente dos fatores que impactam a ploidia, sendo uma base essencial para análises e decisões futuras no estudo.

\paragraph{\textbf{Atividade 4 (A4):} Separar o conjunto de dados em conjuntos de treinamento, validação e teste, fazendo uma distribuição dos dados e aplicar técnica de aumento de dados}
   
A separação do conjunto de dados será feita para garantir que as etapas de treinamento, validação e testes sejam feitas de forma organizada. Iremos dividir em 3 conjuntos: Treinamento, para ensinar o modelo; Validação, para ajustar os parâmetros de forma adequada e evitar o overfitting, ocorre quando um algoritmo reduz o erro por meio da memorização de exemplos de treinamento em vez de aprender a verdadeira relação geral entre os dados \cite{bashir2020}; e Teste, para avaliar o desempenho final. A divisão será produzida com uma distribuição de 70\% dos dados para treinamento, 15\% para validação e 15\% para teste. O nosso conjunto de dados original é relativamente pequeno, tendo 84 linhas, assim, aplicaremos a técnica de aumento de dados (data augmentation) usando o algoritmo Monte Carlo (APÊNDICE \ref{apendice:montecarlo}), que gera novas amostras a partir de dados existentes. Utilizaremos a técnica exclusivamente nos dados de treinamento para evitar interferências ruins na criação e desempenho do modelo de aprendizado de máquina, preservando as características e padrões já existentes na tabela, contribuindo para a capacidade do modelo \cite{kiar2021}. 

Para a realização, usaremos uma abordagem padrão em aprendizado de máquina, dividindo o conjunto de dados em três, como citado. A divisão é feita para garantir que cada conjunto seja representativo e que o modelo seja avaliado imparcialmente \cite{bashir2020}. Para aumentar o conjunto de dados de treinamento, será aplicada uma técnica de aumento de dados (data augmentation) com Monte Carlo, que é a geração de dados artificiais de alta qualidade por meio da manipulação de amostras de dados existentes \cite{wang2024}. 

O resultado esperado é a divisão do conjunto de dados em três subconjuntos: treinamento, validação e teste e a ampliação do conjunto de treinamento com o uso de data augmentation, aplicando o algoritmo de Monte Carlo. 

\begin{table}[h!]
  \centering
  \renewcommand{\arraystretch}{1.0} 
  \captionsetup{font=footnotesize, justification=centering, labelsep=period, position=above}
  \caption{Fase 2: Desenvolvimento e Avaliação do Modelo}
  \label{tab:fase2}
  \resizebox{\textwidth}{!}{ 
    \begin{tabular}{|p{3cm}|p{4cm}|p{3cm}|p{5cm}|} 

      \hline
      \multicolumn{4}{|c|}{\cellcolor[HTML]{008940} \textcolor{white}{\textbf{Fase 2: Desenvolvimento e Avaliação do Modelo}} \vspace{0.2cm}} \\
      \hline
      
      \vspace{0.2cm} \cellcolor[HTML]{C0C0C0}  \textbf{Objetivos Específicos} \vspace{0.2cm} & 
      \vspace{0.2cm} \cellcolor[HTML]{C0C0C0}  \textbf{Atividades} \vspace{0.2cm} & 
      \vspace{0.2cm} \cellcolor[HTML]{C0C0C0}  \textbf{Método de Pesquisa} \vspace{0.2cm} & 
      \vspace{0.2cm} \cellcolor[HTML]{C0C0C0}  \textbf{Resultados Esperados}\vspace{0.2cm} \\
      
      \midrule

      \cellcolor[HTML]{133e78} \vspace{0.2cm} \textcolor{white}{\parbox[c]{\linewidth}{\centering \textbf{OE2 \\ Treinamento e Ajuste de Modelo de Machine Learning para Predição de Euploidia}}} \vspace{0.2cm} & 
      \vspace{0.2cm} \parbox[t]{\linewidth}{\textcolor[HTML]{133E78}{\textbf{Atividade 5 (A5)}} \\ Desenvolvimento e Treinamento do Modelo de Machine Learning para Otimização da Predição de Euploidia, Incluindo Treinamento, Validação e Teste} \vspace{0.2cm} & 
      \vspace{0.2cm} Python (Bibliotecas scikit-learn) \vspace{0.2cm} & 
      \vspace{0.2cm} - Treinamento do modelo bem-sucedido com no mínimo 70\% de precisão, usando KNN, regressão linear e naive bayes \vspace{0.2cm} \\

      \hline

      \vspace{0.2cm} \multirow{2}{*}{\cellcolor[HTML]{133e78} \textcolor{white}{\parbox[c]{\linewidth}{\centering \textbf{OE3 \\ Avaliação do Modelo}}}} \vspace{0.2cm} & 
      \vspace{0.2cm} \parbox[t]{\linewidth}{\textcolor[HTML]{133E78}{\textbf{Atividade 6 (A6)}} \\ Utilizar métricas adequadas para medir o desempenho do modelo} \vspace{0.2cm} & 
      \vspace{0.2cm} Python (Biblioteca scikit-learn e pandas) \vspace{0.2cm} & 
      \vspace{0.2cm} \parbox[t]{\linewidth}{- Acurácia \\ - Precisão \\ - Recall \\ - F1-Score} \vspace{0.2cm} \\

      \cline{2-4}

      \cellcolor[HTML]{133e78} & 
      \vspace{0.2cm} \parbox[c]{\linewidth}{\textcolor[HTML]{133E78}{\textbf{Atividade 7 (A7)}} \\ Avaliação do Desempenho do Modelo na Predição por Meio da Matriz de Confusão e Curva ROC} \vspace{0.2cm} & 
      \vspace{0.2cm} Matriz de confusão (Random Forest - scikit-learn) \vspace{0.2cm} & 
      \vspace{0.2cm} \parbox[t]{\linewidth}{- Verdadeiros positivos (TP) \\ - Verdadeiros negativos (TN) \\ - Falsos positivos (FP) \\ - Falsos negativos (FN) \\ - Gráfico exibindo a relação entre a sensibilidade e a especificidade para diferentes valores de limiar.} \vspace{0.2cm} \\

      \hline

      \cellcolor[HTML]{133e78} \vspace{0.2cm} \textcolor{white}{\parbox[c]{\linewidth}{\centering \textbf{OE4 \\ Protótipo de Interface}}} & 
      \vspace{0.2cm} \parbox[t]{\linewidth}{\textcolor[HTML]{133E78}{\textbf{Atividade 8 (A8)}} \\ Prototipar uma interface} \vspace{0.2cm} & 
      \vspace{0.2cm} Interface básica desenvolvida no FIGMA \vspace{0.2cm} & \vspace{0.2cm} \parbox[t]{\linewidth}{- Interface básica \\ - Coleta de opiniões}\vspace{0.2cm} \\

      \hline 
    \end{tabular}
  }
\end{table} 
\FloatBarrier

\subsubsection{\textbf{Objetivo Específico 2} - Treinamento e Ajuste de Modelo de Machine Learning para Predição de Euploidia}

\paragraph{\textbf{Atividade 5 (A5):} Desenvolvimento e Treinamento do Modelo de Machine Learning para Otimização da Predição de Euploidia, Incluindo Treinamento, Validação e Teste}

O desenvolvimento do modelo de Machine Learning para a predição de euploidia começará com a implementação do algoritmo k-Nearest Neighbors (KNN), com o objetivo de classificar os embriões como euploides ou aneuploides. O KNN foi escolhido inicialmente devido à sua simplicidade e eficácia em problemas de classificação, especialmente quando se tem dados com distribuições bem definidas. Caso o modelo de KNN não forneça os resultados esperados, a regressão linear e o Naive Bayes serão testados como alternativas. O objetivo principal é alcançar uma precisão de 70\%, e se esse objetivo não for atingido com o KNN, esses modelos alternativos serão ajustados até que o objetivo seja atingido.

O processo de desenvolvimento será realizado em várias etapas. Na Atividade 3, o pré-processamento dos dados já terá sido realizado, incluindo a limpeza e normalização dos dados, sendo a normalização especialmente importante para o KNN, pois este modelo é sensível à escala dos dados \cite{zhang2016}. Com os dados prontos, passaremos para a fase de treinamento do modelo.

Inicialmente, utilizaremos o \textit{KNeighborsClassifier} do \textit{Scikit-learn}. O primeiro passo será definir o número de vizinhos (n\_neighbors), um dos parâmetros mais importantes do KNN \cite{zhang2016}. Para isso,  utilizaremos um valor de k igual a 3 para o número de vizinhos, devido à sua simplicidade e aplicabilidade comprovada em problemas semelhantes. Se o modelo não alcançar a precisão de 70\% no conjunto de teste, ajustaremos o valor de k para 5, a fim de observar se uma maior quantidade de vizinhos pode melhorar a desempenho do modelo.

A regressão linear será uma das alternativas a ser explorada. Embora tradicionalmente voltada para problemas de regressão, a regressão linear pode ser adaptada para tarefas de classificação binária  \cite{rodrigues}. Usaremos o \textit{LogisticRegression} do \textit{Scikit-learn} para esse fim. Assim como o KNN, o modelo de regressão linear será treinado com os dados de treinamento, e seus parâmetros serão ajustados, se necessário, para maximizar a precisão. A avaliação será feita com base nos resultados obtidos no conjunto de teste.

Outra alternativa será o Naive Bayes, que é particularmente eficiente para problemas de classificação, especialmente quando as características são independentes entre si  \cite{rish2001}. Usaremos a implementação do \textit{NaiveBayes} disponível no \textit{Scikit-learn}, treinando o modelo da mesma forma que os anteriores, e ajustando seus parâmetros conforme necessário.

Os dados utilizados para treinar e testar os modelos serão divididos em três conjuntos: treinamento, validação e teste. O conjunto de treinamento será utilizado para ajustar os parâmetros do modelo, o de validação ajudará a ajustar os hiperparâmetros como o valor de k no KNN, e o conjunto de teste será usado para avaliar a precisão final do modelo.

A avaliação final do modelo será feita na Atividade 6, onde a precisão dos diferentes modelos será comparada. Nosso objetivo é alcançar uma precisão de 70\% na classificação dos embriões como euploides ou aneuploides. Caso isso não seja alcançado com o KNN, a regressão linear e o Naive Bayes serão testados até que o modelo que melhor se ajuste aos dados seja encontrado, com a precisão desejada.

Ao final, espera-se que o modelo final seja capaz de classificar corretamente os embriões com pelo menos 70\% de precisão, permitindo uma análise confiável da euploidia, que pode ser utilizada como apoio em estudos científicos e aplicações clínicas.

\subsubsection{\textbf{Objetivo Específico 3} - Avaliação do modelo}

\paragraph{\textbf{Atividade 6 (A6):} Utilizar métricas de avaliação mais adequadas para medir o desempenho do modelo de acordo com a natureza do problema de classificação}

O objetivo dessa atividade é aplicar e avaliar métricas adequadas para medir a confiança e o desempenho do modelo de IA, considerando as especificidades e objetivos do problema abordado. As métricas escolhidas incluem Acurácia, Precisão, Recall (Sensibilidade) e F1-Score Cada métrica será implementada e analisada com base em um conjunto de dados previamente definido, utilizando ferramentas de análise estatística e frameworks de aprendizado de máquina. A análise será realizada para garantir que as métricas reflitam de forma eficaz o desempenho do modelo em termos de sua capacidade de generalização e identificação de padrões no conjunto de teste.

A definição e cálculo das métricas de avaliação serão conduzidos utilizando \textit{Python} e bibliotecas amplamente empregadas em aprendizado de máquina, como \textit{scikit-learn}, \textit{pandas}, para análise, visualização e manipulação dos resultados. O processo será realizado para fornecer uma análise detalhada e equilibrada do classificador. As métricas escolhidas incluem acurácia, precisão, \textit{recall} e \textit{F1-Score}. A acurácia mede a proporção de previsões corretas em relação ao total de previsões realizadas \cite{vilela2022}. Essa métrica é útil para problemas onde as classes estão balanceadas e não há uma preocupação maior com erros específicos, como falsos positivos ou falsos negativos \cite{vilela2022}. Ela será calculada usando a função \textit{accuracy\_score} do \textit{scikit-learn}. A precisão avalia a proporção de exemplos classificados como positivos que realmente pertencem à classe positiva \cite{vilela2022}. É crucial em problemas onde falsos positivos têm consequências severas, como em diagnósticos médicos \cite{vilela2022}. Será calculada com a função precision\_score do \textit{scikit-learn}. O \textit{recall} mede a capacidade do modelo de identificar corretamente os exemplos pertencentes à classe positiva \cite{vilela2022}. É especialmente importante em situações onde falsos negativos têm maior impacto \cite{vilela2022}, como na detecção de aneuploidia de embriões. Será calculado com a função \textit{recall\_score} do \textit{scikit-learn}. O \textit{F1-Score} combina precisão e \textit{recall}, fornecendo uma visão equilibrada entre ambos \cite{vilela2022}. É particularmente relevante em casos onde as classes estão desbalanceadas e há necessidade de avaliar o desempenho geral do modelo \cite{vilela2022}. O cálculo será realizado utilizando a função \textit{f1\_score} do \textit{scikit-learn}.

Espera-se que a análise detalhada das métricas forneça uma visão abrangente do desempenho do modelo, destacando seus pontos fortes e fracos em diferentes cenários. A Acurácia deverá apresentar um panorama geral da desempenho, enquanto Precisão, Recall e F1-Score deverão evidenciar aspectos específicos de classificação positiva e negativa \cite{vilela2022}. A métrica \textit{ROC-AUC} permitirá avaliar a capacidade geral do modelo de distinguir entre classes \cite{vilela2022}. Os resultados deverão ser utilizados para refinar e ajustar o modelo, garantindo maior confiabilidade e aderência aos objetivos propostos no problema de classificação. Além disso, a escolha criteriosa das métricas será essencial para orientar decisões estratégicas relacionadas ao modelo, especialmente em aplicações sensíveis a erros de classificação.

\paragraph{\textbf{Atividade 7 (A7):} Avaliar a precisão e eficácia do modelo em prever corretamente casos de euploidia e aneuploidia  por meio da Matriz de Confusão e Curva ROC}

O objetivo desta atividade é avaliar o desempenho do modelo de classificação desenvolvido para prever corretamente os casos de euploidia e aneuploidia, utilizando a Matriz de Confusão e a Curva ROC (Receiver Operating Characteristic). A Matriz de Confusão será construída após o treinamento e teste do modelo, permitindo identificar as taxas de verdadeiros positivos, falsos positivos, verdadeiros negativos e falsos negativos. Essa análise ajudará a compreender o desempenho geral do modelo e a identificar possíveis áreas de melhoria. Adicionalmente, será gerada a Curva \textit{ROC} para avaliar o desempenho do modelo na separação das duas classes: euploide (classe positiva) e aneuploide (classe negativa) \cite{vilela2022}. A curva será analisada com base em diferentes valores de limiar (threshold), fornecendo uma visão detalhada sobre a sensibilidade e a especificidade do modelo em cada ponto. A métrica AUC (Área sob a Curva) será utilizada como indicador global da capacidade do modelo de distinguir entre as classes, sendo especialmente útil para avaliar problemas de classificação desbalanceada \cite{vilela2022}.

A avaliação do desempenho do modelo será realizada em duas etapas principais: a construção da Matriz de Confusão e a geração da Curva ROC. A Matriz de Confusão é uma ferramenta essencial para entender o desempenho do modelo de classificação. De acordo com \citeonline{sathyanarayanan2024}, essa matriz é uma tabela de dimensão N x N, onde N representa o número de classes. Cada linha da matriz indica a quantidade de instâncias previstas em uma classe, enquanto cada coluna representa a quantidade de instâncias reais da classe. Para este estudo, a matriz permitirá a análise de predições corretas e incorretas, classificando-as como verdadeiros positivos (TP), verdadeiros negativos (TN), falsos positivos (FP) e falsos negativos (FN). 

A partir dessas classificações, podemos calcular diversas métricas importantes para medir a precisão do modelo, como a acurácia, precisão, \textit{recall} e \textit{F1-score}, que nos ajudam a identificar as áreas de melhoria no modelo de predição. Além disso, será utilizada uma Curva \textit{ROC} para avaliar o desempenho do modelo de forma mais detalhada. O modelo de classificação Random Forest será implementado utilizando a biblioteca \textit{scikit-learn}, que permite gerar probabilidades de pertença à classe positiva (euploide). A Curva ROC, que é fundamental para problemas de classificação binária, é baseada nessas probabilidades, em vez de apenas classificações binárias \cite{vilela2022}. 

A curva será gerada variando o limiar de decisão, (\textit{threshold}), do modelo. O limiar define a probabilidade a partir da qual uma instância será classificada como pertencente à classe positiva (euploide) \cite{vilela2022}. O threshold será ajustado para diferentes valores, e para cada um, será calculada a sensibilidade (taxa de verdadeiros positivos) e a especificidade (1 - taxa de falsos positivos) \cite{vilela2022}. Para isso, utilizaremos o método predict\_proba() do \textit{scikit-learn} para obter as probabilidades previstas pelo modelo. A função \textit{roc\_curve()} da biblioteca também será utilizada para calcular os valores de falso positivo e verdadeiro positivo para os diferentes limiares, gerando o gráfico da Curva \textit{ROC}, com a especificidade no eixo x e a sensibilidade no eixo y. 

Finalmente, a métrica \textit{AUC} (Área sob a Curva) será calculada utilizando a função \textit{roc\_auc\_score()} do \textit{scikit-learn}. A \textit{AUC}, que varia de 0 a 1, fornecerá uma avaliação quantitativa do modelo, sendo que valores mais próximos de 1 indicam um melhor desempenho na classificação. A Curva \textit{ROC}, junto com a \textit{AUC}, nos ajudará a entender o comportamento do modelo para diferentes limiares e a selecionar o melhor ponto de corte, equilibrando os \textit{trade-offs} entre a taxa de verdadeiros positivos e falsos positivos.

Ao construir a Matriz de Confusão, espera-se obter uma análise detalhada do desempenho do modelo, identificando os verdadeiros positivos (TP), verdadeiros negativos (TN), falsos positivos (FP) e falsos negativos (FN). Essa análise permitirá avaliar não apenas a precisão global do modelo, mas também as taxas de erro em diferentes categorias, como os casos de euploidia erroneamente classificados como aneuploidia (FP) e os de aneuploidia erroneamente classificados como euploidia (FN). Essa avaliação fornecerá subsídios para aprimoramentos no modelo de classificação. Para a Curva ROC, o gráfico gerado mostrará a relação entre a sensibilidade e a especificidade em diferentes valores de limiar. Espera-se que o modelo apresente uma curva ascendente, indicando sua capacidade de identificar corretamente os positivos (euploide) sem gerar muitos falsos positivos. O valor da AUC (Área sob a Curva) será calculado para quantificar a habilidade do modelo em distinguir entre as classes \cite{vilela2022}. Valores de AUC próximos de 1 indicam um excelente desempenho do modelo, enquanto valores próximos de 0,5 sugerem que o modelo apresenta desempenho similar a uma escolha aleatória \cite{vilela2022}. A partir da Curva ROC, será possível selecionar o limiar mais adequado para balancear os erros de falso positivo e falso negativo, permitindo uma análise criteriosa dos trade-offs envolvidos \cite{vilela2022}. Esses resultados fornecerão uma base sólida para avaliar a eficácia do modelo e sua aplicabilidade no contexto do estudo.

\subsubsection{\textbf{Objetivo Específico 4} - Protótipo de Interface}

\paragraph{\textbf{Atividade 8 (A8):} Prototipar uma interface básica para exibir as predições de euploidia para o usuário final (médicos)}

A finalidade é desenvolver o protótipo de uma interface básica que possibilite aos médicos visualizar as previsões de euploidia produzidas pelo modelo. O protótipo incluirá componentes cruciais como campos de preenchimento para os dados necessários à previsão, botões de interação para envio de informações, além de uma área de apresentação dos resultados.

Para criar uma interface fácil de usar que permita aos médicos visualizar as previsões de euploidia, empregaremos o Figma, um software colaborativo baseado na web para a criação de interfaces. O Figma disponibiliza funcionalidades que simplificam a criação de interfaces de usuário e experiências do usuário, priorizando a colaboração em tempo real, por meio de uma gama de ferramentas de edição vetorial e prototipagem \cite{figma2024}.

O resultado é que o protótipo da interface básica possibilite aos médicos uma visualização clara e intuitiva das previsões de euploidia. A interface precisa ser funcional e esteticamente atraente, assegurando a compreensão simples dos resultados exibidos. Serão coletadas opiniões valiosas sobre o design, a navegação e a clareza das informações apresentadas, possibilitando as alterações necessárias.

\section{Passos para o Desenvolvimento de um Algoritmo de Aprendizado de Máquina}

O desenvolvimento de sistemas baseados em aprendizado de máquina é um processo que envolve uma série de etapas bem definidas, cada uma desempenhando um papel crucial para o sucesso do modelo final. Essas etapas vão desde a definição inicial do problema até a implementação e manutenção do sistema em um ambiente de produção. Segundo \citeonline{geron2017}, em \textit{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow}, seguir uma abordagem estruturada permite que os desenvolvedores abordem os desafios de forma sistemática, garantindo não apenas a eficácia técnica do modelo, mas também sua aplicabilidade prática. O processo começa com a definição do problema e o entendimento do contexto geral. Depois, os dados são coletados, explorados para descobrir padrões e preparados para melhorar os resultados dos algoritmos. Em seguida, diferentes modelos são testados, e os melhores são ajustados para formar uma solução final. Essa solução é apresentada, implementada em produção e monitorada continuamente para garantir que funcione bem ao longo do tempo \cite{geron2017}.

\subsection{Definição do problema e análise do panorama geral}
A definição clara do problema e a análise do panorama geral são etapas essenciais para o sucesso de projetos de aprendizado de máquina, pois orienta todas as decisões subsequentes, desde a coleta de dados até a implementação final. Conforme abordado por \citeonline{geron2017} e \citeonline{muller2017}, essas etapas fornecem a base para decisões estratégicas ao longo do desenvolvimento do modelo.  Isso envolve determinar como a solução será utilizada, identificar o tipo de problema (como classificação ou regressão) e escolher métricas de desempenho que estejam alinhadas aos objetivos esperados \cite{geron2017}. O primeiro passo é estabelecer claramente o objetivo em termos de negócio e pesquisa. 

Além disso, é importante avaliar as soluções existentes ou alternativas em uso, que podem servir como ponto de comparação para medir o impacto do modelo. Também é necessário validar hipóteses iniciais sobre os dados e a abordagem, garantindo que o problema esteja bem estruturado e que as limitações sejam compreendidas desde o início. Segundo \citeonline{muller2017}, uma compreensão profunda dos dados e suas características é essencial para a escolha dos algoritmos e para o sucesso do projeto. Perguntas como “Quantos dados possuo?”, “Há dados faltantes?” e “Esses dados são suficientes para responder às perguntas do projeto?” guiam essa análise \cite{muller2017}.

Por fim, essa etapa também exige atenção ao alinhamento entre o problema técnico e os resultados esperados em termos de negócio ou impacto social. Isso garante que o desenvolvimento não seja apenas tecnicamente sólido, mas também relevante e eficaz em seu contexto de aplicação.

\subsection{Obtenção de Dados}
A etapa de obtenção de dados é um dos pilares fundamentais para o sucesso de um projeto de aprendizado de máquina, pois a qualidade e a relevância das informações coletadas impactam diretamente a eficácia do modelo \cite{geron2017}. O processo começa com a identificação e a listagem dos dados necessários, levando em conta sua quantidade e suas características, como formato, tipo e origem \cite{geron2017}. É igualmente importante garantir que as fontes de dados sejam documentadas, que haja espaço suficiente para armazenamento e que os dados estejam acessíveis de maneira eficiente.

Nessa etapa, é imprescindível observar as obrigações legais e éticas, especialmente as previstas na Lei Geral de Proteção de Dados (LGPD) no Brasil. Isso abrange a obtenção de consentimentos adequados para o uso das informações e a aplicação de medidas de segurança, como a anonimização, para proteger dados sensíveis \cite{muller2017}. Conforme destacado por \citeonline{muller2017}, além de cumprir os requisitos legais, é fundamental assegurar a integridade e a privacidade dos dados, principalmente em situações que envolvam informações pessoais ou confidenciais.

\citeonline{geron2017} aborda que a qualidade dos dados é fundamental para o sucesso de qualquer projeto de Machine Learning. Modelos treinados com dados insuficientes, não representativos ou de baixa qualidade tendem a apresentar problemas como overfitting, underfitting ou generalizações imprecisas. Dados não representativos podem introduzir vieses que prejudicam a aplicação do modelo em cenários reais, enquanto dados de baixa qualidade, como informações inconsistentes, incompletas ou ruidosas, comprometem diretamente a capacidade do modelo de identificar padrões úteis.

\citeonline{geron2017} também destaca que a coleta e o preparo dos dados vão além do aspecto técnico. Essas atividades envolvem práticas organizacionais, como a colaboração entre equipes de negócios e engenharia para identificar as fontes de dados mais relevantes, e uma análise cuidadosa para garantir que os dados refletem a realidade do problema a ser resolvido. Além disso, questões legais e éticas desempenham um papel central, principalmente em situações que envolvam informações sensíveis ou pessoais. A conformidade com legislações como a GDPR na Europa e a LGPD no Brasil não apenas protege os dados, mas também promove a confiança dos usuários e das partes interessadas \cite{geron2017}.

Para mitigar os desafios relacionados à qualidade dos dados, \citeonline{geron2017} sugere o uso de técnicas como a limpeza, normalização e engenharia de atributos. Além disso, ele ressalta a importância de práticas contínuas de validação, como a separação adequada entre conjuntos de treinamento, validação e teste, garantindo que o modelo seja testado em dados que nunca encontrou antes. Dessa forma, a abordagem holística para o gerenciamento de dados reforça o desenvolvimento de sistemas robustos, confiáveis e éticos em projetos de Aprendizado de Máquina \cite{geron2017}.

\subsection{Exploração de Dados}
A obtenção e a exploração de dados representam etapas essenciais para o sucesso de projetos de aprendizado de máquina, pois determinam a base sobre a qual os modelos serão desenvolvidos \cite{geron2017}. Antes de preparar os dados para os algoritmos, é essencial entender as características e as relações existentes no conjunto de dados, iniciando a exploração com uma cópia dos dados originais, reduzindo sua escala, se necessário, para facilitar a análise inicial \cite{geron2017}.

A exploração dos dados deve seguir uma abordagem sistemática que envolve:

\begin{enumerate}
    \item \textbf{Estudo das características dos atributos:} Identificar o nome, o tipo (categórico, numérico, texto, etc.), o percentual de valores ausentes, o nível de ruído (outliers, erros de arredondamento), a utilidade potencial para a tarefa e o tipo de distribuição (gaussiana, uniforme, logarítmica) dos dados disponíveis.
    \item \textbf{Identificação de atributos-alvo:} No caso de aprendizado supervisionado, determinar qual atributo será o alvo da predição.
    \item \textbf{Visualização de dados:} Criar gráficos de dispersão, histogramas ou outros métodos visuais para identificar padrões, correlações e tendências. \citeonline{geron2017} sugere, por exemplo, experimentar combinações de atributos, como comparar o número de quartos por domicílio, em vez de analisar apenas o número total de quartos.
    \item \textbf{Correlação entre atributos:} Analisar as relações entre as variáveis para identificar combinações promissoras que possam melhorar a precisão do modelo.
\end{enumerate}

\citeonline{muller2017} destaca que a inspeção visual dos dados é essencial para compreender sua estrutura e identificar inconsistências, como unidades de medida divergentes ou valores inesperados, comuns em cenários reais. Ele recomenda o uso de gráficos de dispersão para analisar relações entre dois atributos ou gráficos de pares para explorar múltiplas combinações quando o número de variáveis é pequeno. Essa etapa também permite verificar se o problema pode ser resolvido manualmente, validando se as informações necessárias estão presentes no conjunto de dados.

Além disso,\citeonline{geron2017} ressalta a importância de aplicar transformações aos atributos, criando variáveis derivadas mais relevantes, como "população por domicílio", em vez de usar dados brutos. Ele também enfatiza a necessidade de documentar aprendizados e, se necessário, ajustar o escopo do projeto para incluir dados adicionais que possam melhorar os resultados. Essas práticas tornam a análise de dados uma etapa crucial para preparar modelos robustos e aumentar as chances de sucesso no projeto.

\subsection{Preparação dos dados para os Algoritmos de Aprendizado de Máquina}
A preparação dos dados para algoritmos de aprendizado de máquina é uma etapa essencial que envolve diversas transformações. Para tornar o processo mais eficiente, é recomendável criar funções específicas para realizar essas transformações, o que permite aplicá-las facilmente em novos conjuntos de dados e reutilizá-las em projetos futuros \cite{geron2017}. Entre as principais transformações, destaca-se a limpeza dos dados, que envolve lidar com valores ausentes. Para atributos com valores ausentes existem três opções: eliminar os registros correspondentes, excluir o atributo inteiro ou substituir os valores faltantes por uma constante, como a média ou a mediana \cite{geron2017}. 

Outro aspecto crucial na preparação dos dados é a escalabilidade dos atributos. Muitos algoritmos de aprendizado de máquina não funcionam bem quando as variáveis numéricas têm escalas muito diferentes \cite{geron2017}. Recomenda-se o uso de técnicas de escalonamento de atributos, como a normalização (min-max scaling), que ajusta os valores para um intervalo de 0 a 1, ou a padronização (standardization), que ajusta os dados para ter média zero e variância unitária \cite{geron2017}. A escolha entre essas duas técnicas depende das características do algoritmo utilizado, já que a padronização é menos afetada por outliers e pode ser mais indicada em certos casos, como em redes neurais. A partir desse processo de preparação, o próximo passo é a seleção de modelos promissores, onde o uso de validação cruzada e a análise dos erros dos modelos ajudam a refinar a escolha do modelo mais adequado para o problema em questão \cite{geron2017}.

\subsection{Seleção e treinamento do modelo}
A seleção e o treinamento de modelos em aprendizado de máquina são etapas essenciais para desenvolver soluções eficazes \cite{geron2017}. Após a preparação dos dados, que inclui a exploração e a transformação, o próximo passo é escolher e treinar um modelo adequado. Para isso, um modelo simples como a \textit{Regressão Linear} pode ser treinado no conjunto de dados, permitindo observar sua performance inicial \cite{geron2017}. No entanto, para uma avaliação mais precisa, a validação cruzada é uma abordagem melhor. Essa técnica divide o conjunto de treinamento em K subconjuntos e, em seguida, treina e avalia o modelo várias vezes, usando um subconjunto diferente para validação a cada vez, o que gera uma estimativa mais confiável da performance do modelo \cite{geron2017}.

Uma alternativa eficaz para melhorar o desempenho do modelo é utilizar \textit{Random Forests}, que combinam múltiplas árvores de decisão para melhorar a acurácia e reduzir o risco de overfitting \cite{geron2017}. O aprendizado de conjunto (ensemble learning) é uma técnica poderosa que utiliza a combinação de diversos modelos para aumentar a robustez e a precisão do modelo final \cite{geron2017}. A abordagem de treinar múltiplos modelos com parâmetros padrão e avaliá-los usando validação cruzada permite selecionar as melhores opções para o problema em questão \cite{geron2017}.

Além disso, para aprimorar a performance do modelo, é fundamental realizar uma análise das variáveis mais relevantes e ajustar as características dos dados \cite{geron2017}. A engenharia de atributos e a seleção de features permitem que os modelos se ajustem para cometer diferentes tipos de erros, o que ajuda a melhorar a precisão geral. Essas etapas devem ser repetidas de forma iterativa, ajustando o modelo com base nas análises de erros e nas mudanças nos dados, o que possibilita a evolução do desempenho do modelo ao longo do processo \cite{geron2017}.

No caso de problemas de classificação, o \textit{algoritmo k-vizinhos mais próximos (k-NN}) é uma das opções mais simples e eficazes \cite{muller2017}. Esse modelo faz previsões baseadas na proximidade dos dados de treinamento, atribuindo o rótulo da classe mais comum entre os vizinhos mais próximos de um ponto desconhecido \cite{muller2017}. A definição do parâmetro 'k', que determina quantos vizinhos são considerados, pode ser ajustada para otimizar os resultados. A implementação do \textit{KNeighborsClassifier} no \textit{Scikit-learn} facilita a criação e a avaliação do modelo, tornando-o uma excelente escolha para tarefas de classificação simples \cite{muller2017}.

\subsection{Ajuste do modelo}

O processo de ajuste fino de modelos (fine-tuning) é uma etapa crucial no desenvolvimento de modelos de aprendizado de máquina, especialmente após a seleção de modelos promissores \cite{geron2017}. Uma das abordagens mais comuns para realizar esse ajuste é o uso de \textit{GridSearchCV do Scikit-Learn}, que automatiza a busca pelos melhores hiperparâmetros do modelo. Em vez de testar manualmente combinações de valores para os hiperparâmetros, o \textit{GridSearchCV} avalia todas as possibilidades de uma lista de valores, utilizando validação cruzada para escolher a melhor configuração \cite{geron2017}. Isso facilita o processo, economizando tempo e aumentando a precisão na escolha dos parâmetros ideais para o modelo.

No entanto, para espaços de busca de hiperparâmetros maiores, a \textit{RandomizedSearchCV} pode ser uma alternativa mais eficiente. Em vez de testar todas as combinações possíveis, essa abordagem realiza uma busca aleatória, selecionando valores aleatórios para cada hiperparâmetro em cada iteração \cite{geron2017}. Essa técnica oferece duas grandes vantagens: a possibilidade de explorar um maior número de combinações de hiperparâmetros dentro de um orçamento computacional limitado, além de permitir um controle mais flexível sobre o número de iterações realizadas (Géron, 2017). Dessa forma, a \textit{RandomizedSearchCV} é especialmente útil quando o espaço de busca é grande e as combinações possíveis são muitas \cite{geron2017}.

Além do ajuste de hiperparâmetros, outra técnica importante para aprimorar o modelo é o uso de \textit{Métodos de Ensemble}. Esses métodos combinam os melhores modelos individuais, muitas vezes resultando em um desempenho superior ao de qualquer modelo isolado \cite{geron2017}. A combinação de modelos com erros diferentes pode reduzir a variabilidade e melhorar a precisão geral. Um exemplo clássico é o \textit{Random Forest}, que utiliza múltiplas árvores de decisão para obter melhores resultados do que uma única árvore. A estratégia de ensemble pode ser fundamental para melhorar o desempenho do modelo, especialmente em tarefas complexas \cite{geron2017}.

Após o ajuste fino, é importante analisar os melhores modelos e seus erros para entender melhor o desempenho do sistema. Inspecionar os atributos mais importantes para a previsão pode revelar insights valiosos sobre o problema \cite{muller2017}. Além disso, entender os tipos de erros cometidos pelo modelo e as razões por trás deles pode ajudar a ajustar o modelo, adicionando ou removendo features, tratando outliers ou refinando a transformação dos dados \cite{muller2017}. Finalmente, após realizar todas essas melhorias, o modelo deve ser avaliado no conjunto de teste para estimar sua capacidade de generalização. A avaliação no conjunto de teste fornece uma medida objetiva da performance do modelo em dados não vistos, sendo fundamental para garantir que o modelo não esteja superajustado aos dados de treinamento \cite{muller2017}.

\subsection{Lançamento da Solução}
Após a aprovação do lançamento de um projeto, é crucial preparar a solução para produção, conectando as fontes de dados de entrada e escrevendo os testes necessários para garantir que o sistema funcione conforme esperado. A integridade e a qualidade do sistema, bem como sua adaptação ao ambiente de produção, são essenciais para que ele se mantenha eficiente. Além disso, é necessário implementar códigos de monitoramento que acompanhem a performance do sistema em tempo real, verificando se ele continua operando de maneira eficiente e acionando alertas sempre que houver uma queda na performance \cite{geron2017}. Isso é especialmente importante, pois os modelos de aprendizado de máquina podem sofrer uma degradação gradual ao longo do tempo devido à mudança nos dados, o que é conhecido como "data drift" \cite{geron2017}.

Para garantir a qualidade das previsões, o monitoramento também deve incluir uma avaliação humana periódica, muitas vezes realizada por analistas ou por meio de plataformas de \textit{crowdsourcing}, como o \textit{Amazon Mechanical Turk ou o CrowdFlower}. Essa análise ajuda a validar se o modelo continua atendendo às expectativas e fornece informações cruciais sobre possíveis melhorias  \cite{muller2017}. A qualidade dos dados de entrada também deve ser constantemente monitorada, já que problemas como sensores defeituosos ou dados desatualizados podem impactar significativamente a acuracidade do modelo, prejudicando sua performance e a confiança nos resultados gerados  \cite{muller2017}.

A manutenção do sistema é outro aspecto crítico após o lançamento, exigindo que o modelo seja re-treinado regularmente com dados frescos \cite{geron2017}. Isso é importante para evitar que a performance do sistema flutue de forma inesperada, o que pode acontecer se o modelo for atualizado de maneira esporádica \cite{geron2017}. A automação desse processo de re-treinamento é essencial para garantir que o modelo seja atualizado sempre que necessário, sem depender de intervenções manuais. 

Portanto, o lançamento de um sistema de aprendizado de máquina não se limita à integração inicial dos dados e à validação do modelo. O sucesso contínuo depende de um monitoramento eficiente e de uma manutenção constante, garantindo que o modelo se adapte às mudanças nos dados ao longo do tempo. Implementando uma infraestrutura de monitoramento e re-treinamento robusta, as equipes podem assegurar que o sistema continue atendendo aos objetivos de negócios de forma eficaz e precisa, minimizando riscos e maximizando a performance ao longo de sua vida útil.