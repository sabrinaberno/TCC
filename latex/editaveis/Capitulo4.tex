\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green!50!black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    showstringspaces=false,
    breaklines=true,
    frame=single,
    captionpos=b,
    tabsize=4,
    literate={á}{{\'a}}1 {â}{{\^a}}1 {ã}{{\~a}}1 {é}{{\'e}}1 {ê}{{\^e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ô}{{\^o}}1 {õ}{{\~o}}1 {ú}{{\'u}}1 {ç}{{\c{c}}}1
}

\chapter[Execução da Pesquisa]{Execução da Pesquisa}

Antes de começar a "Execução da Pesquisa e Análise dos Resultados", é crucial enfatizar que todas as fases deste estudo foram realizadas de acordo com as normas éticas e legais pertinentes à investigação científica. Os dados utilizados neste estudo foram obtidos por meio do especialista em Reprodução Humana Assistida, Dr. Bruno Ramalho, e de suas pacientes, que autorizaram o uso dessas informações para fins de pesquisa. As informações foram coletadas na clínica Bruno Ramalho Reprodução Humana, bem como na clínica Genesis, onde o Dr. Bruno também atua. A documentação que comprova isso está devidamente incluída nos anexos deste estudo, incluindo os seguintes documentos essenciais:
\begin{itemize}
  \item \textbf{Parecer da Plataforma Brasil}: Inclui a permissão ética que valida a utilização dos dados dos embriões para a execução deste estudo, garantindo a conformidade com os padrões éticos definidos para pesquisas que envolvem seres humanos (disponível no Anexo \ref{anexo:parecer-cep})
  \item \textbf{Contrato de Autorização para Utilização de Dados em Pesquisa}: Assinado pelo Dr. Bruno Ramalho, que autoriza o uso dos dados clínicos de suas pacientes, essenciais para a modelagem e análise realizadas durante esta pesquisa (disponível no Anexo \ref{anexo:contrato-autorizacao})
  \item \textbf{Termo de Consentimento para Utilização de Dados de Entrevistas, Gravação de Reuniões e Uso de Gravação}: Contrato que permite o uso de discursos, dados e gravações provenientes das reuniões e entrevistas conduzidas com o Dr. Bruno Ramalho, garantindo que as informações e contribuições fornecidas por ele fossem utilizadas com total autorização (disponível no Anexo \ref{anexo:termo-consentimento}).
\end{itemize}
Estes documentos evidenciam o comprometimento ético e a transparência desta pesquisa, enfatizando que todas as informações empregadas foram obtidas de maneira responsável e autorizada.
Os códigos mencionados nas atividades estão disponíveis no repositório do projeto no GitHub, podendo ser acessado aqui: \textbf{\href{https://github.com/sabrinaberno/TCC1}{GitHub-TCC}}, onde são detalhadamente explicados, incluindo a lógica utilizada para sua implementação e os resultados obtidos.




\section{Fase 1: Análise e Preparação de Dados}
\subsection{OE1 - Expansão, Processamento e Análise de Dados para Predição de Ploidia}
\subsubsection{Atividade 1 (A1): Análise, Revisão, Seleção e Limpeza de Variáveis para Predição de Euploidia}
O desenvolvimento desta atividade teve início com a verificação da relevância das variáveis com base nos estudos do referencial teórico. Esse processo envolveu a análise das variáveis presentes na tabela de dados que serão utilizadas nesta pesquisa, garantindo sua adequação ao objetivo do estudo. As variáveis selecionadas inicialmente são:
\begin{table}[ht]
  \centering
  \begin{tabular}{|l|c|}
  \hline
  \textbf{Variáveis} & \textbf{Definição} \\ \hline
  Id & Identificador numérico de cada paciente. \\ \hline
  Idade & Idade da paciente no momento do procedimento. \\ \hline
  Data da biopsia & Data em que foi realizada a biópsia embrionária. \\ \hline
  Embrião n. & Identificação numérica do embrião dentro do ciclo de fertilização. \\ \hline
  Estágio & Dia de evolução no cultivo. \\ \hline
  Morfo & Classificação morfológica dos embriões baseada no estágio de expansão da blástula, estágio inicial do desenvolvimento embrionário. \\ \hline
  Kidscore & Algoritmo que combina variáveis morfocinéticas e parâmetros de desenvolvimento embrionário. \\ \hline
  st2 & Primeiro indício de movimentos citoplasmáticos antes da primeira citocinese. \\ \hline
  t2 & Tempo para 2 células. \\ \hline
  t3 & Tempo para 3 células. \\ \hline
  t4 & Tempo para 4 células. \\ \hline
  t5 & Tempo para 5 células. \\ \hline
  t8 & Tempo para 8 células. \\ \hline
  tSC & Tempo de formação do estágio de clivagem sincronizada (Time to Synchronized Compaction). \\ \hline
  tSB & Tempo para o início da blastulação (Time to Start Blastulation). \\ \hline
  tB & Tempo para o Blastocisto (Time to Blastocyst). \\ \hline
  t2-st2 & Intervalo de tempo entre o T2 e o ST2. \\ \hline
  cc2 (t3-t2) & Tempo necessário para que o embrião passe da divisão de 2 células (T2) para a divisão de 3 células (T3). \\ \hline
  cc3 (t5-t3) & Tempo necessário para que o embrião passe da divisão de 3 células (T3) para a divisão de 5 células (T5). \\ \hline
  t5-t2 & Intervalo de tempo entre o estágio de 2 células (T2) e o estágio de 5 células (T5). \\ \hline
  s2 (t4-t3) & Intervalo de tempo necessário para que o embrião passe do estágio de 3 células (T3) para o estágio de 4 células (T4). \\ \hline
  s3 (t8-t5) & Intervalo de tempo necessário para que o embrião passe do estágio de 5 células (T5) para o estágio de 8 células (T8). \\ \hline
  tSC-t8 & Intervalo de tempo entre o estágio em que o embrião atinge a compactação inicial (tSC) e o estágio de 8 células (T8). \\ \hline
  tB-tSB & Intervalo de tempo entre o estágio em que o embrião atinge o blastocisto inicial (tSB) e o estágio de blastocisto expandido (tB). \\ \hline
  Ploidia & Estado de ploidia do embrião. \\ \hline
  \end{tabular}
  \caption{Variáveis analisadas no modelo de IA para predição de euploidia embrionária. A explicação mais detalhada sobre essas variáveis está no APÊNDICE A}
  \label{tab:definicoes_variaveis}
\end{table}

Com base no estudo do referencial teórico, realizamos uma análise criteriosa para identificar a necessidade de introduzir novas variáveis que possam aprimorar a precisão do modelo, bem como excluir aquelas que se mostraram irrelevantes. A partir da revisão bibliográfica, conduzimos um processo de seleção das variáveis mais relevantes para o nosso modelo de Inteligência Artificial, fundamentado em evidências científicas. Dessa forma, foi possível identificar um conjunto de variáveis extraídas da base de dados que apresentam respaldo teórico quanto à sua influência na evolução do modelo. As variáveis selecionadas para o estudo são: \textbf{Idade, t2, t3, t4, t5, t8, s2, cc2 (t3-t2), tSC, tSB, tB, cc3 (t5-t3), s3 (t8-t5), t5-t2, tSC-t8, tB-tSB, Estágio, Morfo e KIDScore}.

A coluna de \textbf{Plodia} também foi selecionada, pois nos possibilita agrupar os embriões em duas categorias claras, distinguindo entre aqueles com euploidia normal e aqueles com alterações cromossômicas, o que é crucial para a elaboração de um modelo sólido.

Não identificamos estudos que estabelecem uma ligação direta entre o parâmetro \textbf{st2} e a previsão de euploidia. Apesar do movimento citoplasmático antes da citocinese ser um marco significativo no desenvolvimento embrionário, a ausência de provas científicas que liguem esse movimento à qualidade do embrião e à euploidia nos levou a remover o \textbf{st2} da lista de variáveis para o modelo. Igualmente, não foram identificados estudos que analisassem especificamente o intervalo entre \textbf{t2} (o instante em que o embrião alcança a fase de duas células) e \textbf{st2} (movimento citoplasmático pré-citocinese) para prever a euploidia. Como \textbf{st2} foi eliminada, também removemos o parâmetro \textbf{t2-st2} do nosso grupo de variáveis.

A partir disso, conseguimos determinar quais variáveis são fundamentais para a elaboração do nosso modelo de previsão de euploidia. Depois de examinar e revisar as variáveis, modificamos a planilha para espelhar os dados mais significativos para o modelo. As colunas \textbf{Id, Data da biópsia e Embrião n.} também foram eliminadas, uma vez que não contribuem para o valor do modelo. Adicionalmente, as variáveis \textbf{st2} e \textbf{t2-st2} foram eliminadas, conforme mencionado anteriormente. Portanto, a planilha revisada agora inclui somente as variáveis que possuem uma ligação comprovada com a previsão de euploidia, fundamentada nas evidências científicas revisadas.

\subsubsection{Limpeza dos Dados}
Ao lidar com os dados ausentes, utilizamos o método de Análise de Casos Completos (ACC), que envolve a remoção de observações que possuem pelo menos um valor ausente (Camargos, 2011). Este procedimento é frequentemente empregado quando o número de dados ausentes é reduzido, assegurando que a remoção de algumas observações não interfira de forma significativa na análise estatística e preserva a consistência do modelo (Camargos, 2011). Em nosso cenário, das 85 linhas de dados disponíveis, apenas 2 apresentavam valores ausentes. Por esse motivo, excluímos essas duas linhas de modo manual, já que é um número muito pequeno para fazer um código de limpeza de dados, assim, a planilha se modifica, resultando em 83 linhas.

\subsubsection{Atividade 2 (A2): Identificação da Correlação entre os Parâmetros na Previsão da Ploidia do Embrião}

O coeficiente de \textbf{correlação de Spearman}, explicado no APÊNDICE \ref{apendice:spearman}, foi escolhido para esta atividade devido à necessidade de reconhecer relações entre os parâmetros documentados nos dados dos embriões e a porcentagem de euploidia. A metodologia leva em conta a classificação ordinal das observações, minimizando os efeitos de valores atípicos e permitindo a análise de variáveis com distribuições não normais \cite{sousa2019}. Além disso, a sua facilidade de uso em pesquisas científicas e a sua vasta aplicação em estudos destacam sua utilidade na análise de ploidia dos embriões.

O programa em Python criado para essa análise foi desenvolvido para ser modular, eficaz e gerar resultados claros tanto em formato visual quanto textual. Ele emprega as bibliotecas \textit{Pandas, SciPy, Matplotlib e python-docx} para manipular dados, determinar correlações, produzir gráficos e elaborar relatórios em Word. A seguir, detalha-se a lógica do código:

\begin{itemize}
    \item \textbf{Carregamento e Preparação dos Dados}: As informações dos embriões foram processadas a partir da Planilha de Dados Refinada, encontrada no Anexo \ref{anexo:planilha-refinada}, utilizando todas as colunas disponíveis. Essa etapa garante que todas as variáveis relevantes sejam consideradas.
    \item \textbf{Cálculo da Correlação de Spearman}: O coeficiente de Spearman foi calculado para todas as combinações possíveis de variáveis, utilizando a função \texttt{spearmanr} da biblioteca SciPy. O método \texttt{"omit"} foi empregado para lidar com valores ausentes, garantindo maior robustez mesmo que não existam dados faltantes nesta planilha.
    \item \textbf{Criação de Gráficos de Dispersão}: Para cada par de variáveis, foram criados gráficos de dispersão que auxiliam na análise numérica e permitem a identificação visual de padrões. As cores foram padronizadas, com a variável 1 (\texttt{var1}) em verde claro e marcador ``o'', e a variável 2 (\texttt{var2}) em azul escuro e marcador ``x''.
    \item \textbf{Elaboração de Relatório Automatizado}: O relatório gerado contém descrições textuais dos coeficientes e gráficos correspondentes. Este documento automatizado facilita a comunicação visual e escrita dos resultados.
\end{itemize}

O código em Python utilizado para realizar a análise está apresentado a seguir:

\begin{lstlisting}
  import pandas as pd
  from scipy.stats import spearmanr
  import matplotlib.pyplot as plt
  from docx import Document
  from docx.shared import Inches
  import os
  
  file_path = "PlanilhaDadosRefinados.xlsx"
  data = pd.read_excel(file_path)
  
  numerical_columns = [ 
      "Idade", "Estágio", "Morfo", "KIDScore", "t2", "t3", "t4", "t5", "t8", "tSC", "tSB", "tB", "cc2 (t3-t2)", 
      "cc3 (t5-t3)", "t5-t2", "s2 (t4-t3)", "s3 (t8-t5)", "tSC-t8", "tB-tSB", "Ploidia"
  ]
  
  numerical_columns = [col for col in numerical_columns if col in data.columns]
  
  results = []
  for col1 in numerical_columns:
      for col2 in numerical_columns:
          if col1 != col2:  
              coef, _ = spearmanr(data[col1], data[col2], nan_policy="omit")
              results.append({"Variable 1": col1, "Variable 2": col2, "Spearman Coefficient": coef})
  
  correlation_df = pd.DataFrame(results)
  
  output_file = "correlation_results.xlsx"
  correlation_df.to_excel(output_file, index=False)
  print(f"\nResultados salvos no arquivo: {output_file}")
  
  doc = Document()
  doc.add_heading('Correlação de Spearman - Embriões', 0)
  
  output_dir = "gráficos"
  os.makedirs(output_dir, exist_ok=True)
  
  color_var1 = "lightgreen"
  color_var2 = "darkblue"
  
  for index, row in correlation_df.iterrows():
      var1, var2 = row['Variable 1'], row['Variable 2']
      coef = row['Spearman Coefficient']
  
      plt.figure(figsize=(6, 4))
      plt.scatter(data[var1], data[var2], alpha=0.6, c=color_var1, label=f'{var1}', marker='o')
      plt.scatter(data[var2], data[var1], alpha=0.6, c=color_var2, label=f'{var2}', marker='x')
      
      plt.title(f"Dispersão entre {var1} e {var2} (Coeficiente de Spearman: {coef:.2f})")
      plt.xlabel(var1)
      plt.ylabel(var2)
  
      img_path = os.path.join(output_dir, f"grafico_{var1}_{var2}.png")
      plt.savefig(img_path)
      plt.close()
  
      doc.add_heading(f'Correlação entre {var1} e {var2}', level=1)
      doc.add_paragraph(f'Coeficiente de Spearman: {coef:.2f}')
      doc.add_picture(img_path, width=Inches(5.0))
  
  output_word = "relatorio_correlacoes.docx"
  doc.save(output_word)
  
  print(f"\nRelatório gerado e salvo em: {output_word}")
\end{lstlisting}

O código analisa a correlação entre as variáveis numéricas em uma sequência de dados, empregando o coeficiente de Spearman. Ele inicia carregando a planilha Excel com as informações dos embriões e seleciona todas as colunas relevantes para a análise. O coeficiente de Spearman é calculado para cada par de variáveis, avaliando a intensidade e a direção da relação monótona entre elas. Os resultados são armazenados em um \texttt{DataFrame}, que é uma estrutura de dados bidimensional do Pandas, semelhante a uma tabela, permitindo fácil manipulação e análise. Esta tabela é então exportada para um arquivo Excel chamado \texttt{correlation\_results.xlsx}.

Após isso, o programa gera gráficos de dispersão para observar as correlações, ressaltando os padrões das variáveis associadas. Esses gráficos, juntamente com os coeficientes de correlação, são automaticamente incorporados a um documento Word. No final, temos um relatório completo com as análises e visualizações, salvo como um arquivo chamado \texttt{relatorio\_correlacoes.docx}.

\subsubsection{Atividade 3 (A3): Normalização dos Dados para Otimização}
A etapa de normalização dos dados é um passo fundamental na criação de modelos de aprendizado de máquina, particularmente em situações onde as variáveis têm diferentes escalas e distribuições. Esta tarefa foi executada com o uso do método Z-Score. Esta metodologia modifica os dados de forma que cada variável possua uma média de 0 e um desvio padrão de 1, explicado na APÊNDICE \ref{apendice:zscore}. 

A normalização foi conduzida com Python e a biblioteca Pandas. O procedimento foi automatizado para simplificar a análise em grande escala dos dados morfocinéticos dos embriões. A normalização foi aplicada especificamente às variáveis numéricas do conjunto de dados, previamente selecionadas na fase de escolha das variáveis. O código utilizado para realizar essa atividade foi:

\begin{lstlisting}
  import pandas as pd
  import numpy as np
  
  
  file_path = "PlanilhaDeDadosDosEmbriões4.xlsx"  
  data = pd.read_excel(file_path)
  
  
  numeric_columns = data.select_dtypes(include=[np.number]).columns
  
  
  normalized_data = data.copy()
  
  
  for col in numeric_columns:
      mean = data[col].mean()
      std = data[col].std()
      normalized_data[col] = (data[col] - mean) / std
  
  
  output_file = "Planilha_normalizada.xlsx"
  normalized_data.to_excel(output_file, index=False)
  
  
  print(f"Normalização concluída. Arquivo salvo como: {output_file}")
\end{lstlisting}  

A sequência de passos executados pelo código para a normalização dos dados foi:

\begin{enumerate}
    \item \textbf{Análise dos Dados:} Os dados foram inicialmente importados do arquivo Excel que possui os dados dos embriões, usando a função \texttt{pd.read\_excel()}.
    
    \item \textbf{Identificação das Colunas Numéricas:} Em seguida, foram identificadas as colunas numéricas do \textit{DataFrame} através do método \texttt{select\_dtypes()}. Essa etapa é muito importante, já que a normalização com Z-Score deve ser realizada apenas em variáveis contínuas. As variáveis normalizadas foram: Idade, Kidscore, t2, t3, t4, t5, t8, tSC, tSB, tB, cc2 (\texttt{t3-t2}), cc3 (\texttt{t5-t3}), \texttt{t5-t2}, s2 (\texttt{t4-t3}), s3 (\texttt{t8-t5}), \texttt{tSC-t8} e \texttt{tB-tSB}.
    
    \item \textbf{Cálculo da Média e Desvio Padrão:} Para cada variável numérica, a média e o desvio padrão foram calculados. Esses valores são fundamentais para a utilização da fórmula do Z-Score, apresentada e explicada no \textbf{APÊNDICE \ref{apendice:zscore}}.
    
    \item \textbf{Utilização do Z-Score:} Para cada valor de uma variável, a média é subtraída e o desvio padrão é dividido, de acordo com a fórmula. Isso modifica os dados para que todas as variáveis apresentem média zero e desvio padrão igual a 1.
    
    \item \textbf{Armazenamento dos Dados Normalizados:} Depois de aplicar o Z-Score, os dados normalizados foram guardados em uma nova planilha Excel, denominada “\texttt{Planilha\_normalizada.xlsx}”. A planilha está em “\texttt{Atividade 3}” no GitHub do nosso projeto: \textbf{\href{https://github.com/sabrinaberno/TCC1}{GitHub-TCC}}.
\end{enumerate}

A opção pelo Z-Score se deve à sua habilidade de converter os dados para uma escala padrão, preservando as informações pertinentes. Pesquisas apontam que a normalização melhora a precisão dos algoritmos de aprendizado de máquina ao remover os efeitos de escalas distintas entre as variáveis \cite{jaiswal2024}. Além disso, essa técnica simplifica a comparação entre variáveis, auxiliando na interpretação e validação dos modelos criados.

\subsubsection{Atividade 4 (A4): Separar o conjunto de dados em conjuntos de treinamento, validação e teste, fazendo uma distribuição dos dados e aplicar técnica de aumento de dados}
Nessa atividade, apresentamos a implementação do procedimento de segmentação do conjunto de dados em três subconjuntos diferentes: treinamento, validação e teste. Esta distribuição obedece às proporções estabelecidas na metodologia (70\%, 15\% e 15\%, respectivamente) e foi feita através de um \textit{script} em Python. Utilizamos a planilha criada pela Atividade 3, a ``\texttt{Planilha\_normalizada.xlsx}'', pois utilizaremos os dados já normalizados e tratados a partir de agora.

A segmentação do conjunto de dados foi planejada para assegurar que cada subconjunto tenha uma representação adequada para seus respectivos propósitos, com já citado na Capítulo 3 em Metodologia. A distribuição foi feita de maneira aleatória para prevenir viés e assegurar a aplicabilidade geral do modelo. O \textit{script} calculou o tamanho de cada subconjunto com base em um total 82 linhas de amostra, seguindo as porcentagens estabelecidas.

Abaixo, descrevemos o código implementado, seguido de sua explicação detalhada:

\begin{lstlisting}
  import pandas as pd
  import numpy as np
  
  
  df = pd.read_excel("Planilha_normalizada.xlsx")
  
  
  df = df.sample(frac=1, random_state=42).reset_index(drop=True)
  
  
  n_total = len(df)
  n_train = int(n_total * 0.7)
  n_val = int(n_total * 0.15)
  n_test = n_total - n_train - n_val
  
  
  train_data = df.iloc[:n_train]
  val_data = df.iloc[n_train:n_train + n_val]
  test_data = df.iloc[n_train + n_val:]
  
  
  train_data.to_excel("dados_treinamento.xlsx", index=False)
  val_data.to_excel("dados_validacao.xlsx", index=False)
  test_data.to_excel("dados_teste.xlsx", index=False)
  
  
  print(f"Conjunto de Treinamento: {len(train_data)} linhas")
  print(f"Conjunto de Validação: {len(val_data)} linhas")
  print(f"Conjunto de Teste: {len(test_data)} linhas")
\end{lstlisting}  

\begin{itemize}
  \item \textbf{Importação das Bibliotecas:} A biblioteca \texttt{pandas} foi empregada para a manipulação de dados e \texttt{numpy} para realizar operações matemáticas.

  \item \textbf{Carregamento dos Dados:} O documento Excel que continha a base de dados dos embriões foi carregado em um \textit{DataFrame} do \texttt{pandas} para simplificar as operações subsequentes.

  \item \textbf{Embaralhamento dos Dados:} Utilizamos a função \texttt{sample} com os parâmetros \texttt{(frac=1 e random\_state=42)} para embaralhar os dados de maneira aleatória, assegurando a remoção de qualquer padrão sequencial existente no conjunto original. O argumento \texttt{random\_state} garante a reprodutibilidade, possibilitando que o mesmo resultado seja alcançado em futuras execuções.

  \item \textbf{Cálculo das Proporções:} As proporções foram estabelecidas para os conjuntos de treinamento, validação e teste, assegurando que a soma das dimensões corresponda ao número total de amostras. Isso é crucial para prevenir perdas de informações ou desbalanceamento nos conjuntos.

  \item \textbf{Divisão dos Dados:} A escolha dos subconjuntos foi feita com base nos índices do \textit{DataFrame}, que foram divididos em intervalos definidos com base nos tamanhos previamente calculados.

  \item \textbf{Armazenamento dos Dados:} Os subconjuntos resultantes foram salvos em arquivos Excel separados utilizando a função \texttt{to\_excel}, com o parâmetro \texttt{index=False} para evitar a inclusão de índices desnecessários nos arquivos finais. Os arquivos foram nomeados para refletir suas respectivas funções: \texttt{dados\_treinamento.xlsx}, \texttt{dados\_validacao.xlsx} e \texttt{dados\_teste.xlsx}. 
\end{itemize}

O script priorizou a simplicidade, a replicabilidade e a eficácia. Através deste método, garantimos que os conjuntos produzidos sejam apropriados para as fases seguintes de treinamento, validação e teste do modelo de IA, mantendo a integridade dos dados originais.

\subsection*{Aumento de Dados (\textit{Data Augmentation}) utilizando o Método de Monte Carlo:} 
A técnica de aumento de dados (\textit{data augmentation}) foi utilizada para melhorar o desempenho do modelo ao gerar mais exemplos de treinamento a partir dos dados existentes. Este processo visa aumentar a capacidade de generalização do modelo e evitar o \textit{overfitting}, tornando-o mais robusto. Abaixo, descrevemos o código implementado, seguido de sua explicação detalhada:

\begin{lstlisting}
  import pandas as pd
  import numpy as np
  
  
  train_data_path = 'dados_treinamento.xlsx'  
  df_train = pd.read_excel(train_data_path)
  
  
  numerical_columns = df_train.select_dtypes(include=['float64', 'int64']).columns
  categorical_columns = df_train.select_dtypes(include=['object']).columns
  
  
  augmentation_factor = 3  
  num_new_samples = len(df_train) * augmentation_factor
  
  
  new_samples = {}
  for col in numerical_columns:
      mean = df_train[col].mean()
      std = df_train[col].std()
      new_samples[col] = np.random.normal(loc=mean, scale=std, size=num_new_samples)
  
  
  df_new_samples = pd.DataFrame(new_samples)
  
  
  for col in categorical_columns:
      replicated_values = np.random.choice(df_train[col], size=num_new_samples, replace=True)
      df_new_samples[col] = replicated_values
  
  
  df_augmented = pd.concat([df_train, df_new_samples], ignore_index=True)
  
  
  augmented_data_path = 'dados_treinamento_aumentado.xlsx'
  df_augmented.to_excel(augmented_data_path, index=False)
  
  
  print(f"Conjunto de dados aumentado salvo em: {augmented_data_path}")
\end{lstlisting}

O programa emprega distribuições estatísticas autênticas para produzir novas amostras que seguem os padrões dos dados originais. A distinção entre dados numéricos e categóricos possibilita o tratamento adequado de cada variável, garantindo a integridade do conjunto de dados ampliado.

\begin{itemize}
  \item \textbf{Importação das Bibliotecas:} As bibliotecas \texttt{pandas} e \texttt{numpy} foram utilizadas para manipular dados e realizar operações matemáticas: o \texttt{pandas} foi empregado para carregar, manipular e armazenar os conjuntos de dados, enquanto o \texttt{numpy} foi responsável pela criação de valores aleatórios para as colunas numéricas e pela replicação de valores categóricos.

  \item \textbf{Carregamento do Conjunto de Dados de Treinamento:} O documento  com os \texttt{dados de treinamento} foi inserido num \textit{DataFrame}, que atua como uma estrutura para a organização e manipulação eficaz dos dados. A variável \texttt{train\_data\_path} definiu o caminho do arquivo, enquanto a função \texttt{pd.read\_excel} foi empregada para acessar os dados.

  \item \textbf{Identificação das Colunas Numéricas e Categóricas:} Foram escolhidas colunas numéricas (como \texttt{float64} ou \texttt{int64}) para produzir novos valores com base em distribuições estatísticas, enquanto as colunas categóricas (do tipo \texttt{object}) para duplicar valores já existentes, preservando as proporções originais. Esta divisão foi feita por meio dos métodos \texttt{select\_dtypes}, que possibilitaram a identificação e categorização das colunas com base no seu tipo de informação.

  \item \textbf{Definição do Fator de Aumento:} O fator de ampliação foi estipulado em 3, o que sugere que o volume do conjunto de treinamento será triplicado. A quantidade de amostras adicionais foi determinada ao multiplicar o tamanho original do conjunto (\texttt{len(df\_train)}) pelo fator de ampliação. Isso estabeleceu o número de novos exemplos a serem produzidos para ampliar a base de dados de treinamento.

  \item \textbf{Geração de Novas Amostras para Colunas Numéricas:} Para cada coluna numérica, foram criados novos valores por meio da distribuição normal. Os parâmetros utilizados foram o valor médio (\texttt{mean}) e o desvio padrão (\texttt{std}) da coluna original, assegurando que os novos valores mantenham as propriedades estatísticas da distribuição original. Este procedimento garante que os dados expandidos para as colunas numéricas mantenham a mesma distribuição da amostra inicial.

  \item \textbf{Replicação de Valores para Colunas Categóricas:} Os valores já existentes nas colunas categóricas foram replicados de maneira aleatória por meio da função \texttt{np.random.choice}, com reposição. Esta estratégia mantém a proporção original dos valores categóricos, assegurando a uniformidade nos padrões e possibilitando a manutenção da distribuição das categorias nos novos exemplos criados.

  \item \textbf{Combinação dos Dados Originais com os Novos:} A meta era unir os dados originais aos novos dados produzidos, resultando em um conjunto de treinamento expandido. Com o parâmetro \texttt{ignore\_index=True}, a função \texttt{pd.concat} foi empregada para recomeçar os índices no novo conjunto de dados, assegurando que o \textit{DataFrame} gerado apresentasse uma sequência ininterrupta de índices, sem duplicações ou sobreposições.

  \item \textbf{Armazenamento do Conjunto de Dados Ampliado:} O arquivo Excel com os dados ampliados foi nomeado como \texttt{dados\_treinamento\_aumentado.xlsx}. O parâmetro \texttt{index=False} foi empregado para prevenir a criação de índices nos arquivos finais, facilitando sua utilização em fases subsequentes do trabalho e assegurando que o arquivo gerado contenha apenas os dados, sem colunas de índice adicionais.
\end{itemize}

A aplicação da média e do desvio padrão assegura que os novos dados numéricos correspondam às características do conjunto original, ao mesmo tempo que as categorias mantêm suas proporções, mantendo os padrões originais. A produção de dados por meio de distribuições estatísticas amplia a diversidade do conjunto sem adicionar ruído excessivo ou padrões inverossímeis, assegurando uma diversificação controlada. Este procedimento garante que o conjunto de treinamento ampliado seja robusto o suficiente para treinar o modelo, prevenindo sobrecargas e aprimorando a generalização, levando a um modelo mais eficiente e seguro.

\subsubsection{Nota sobre a Apresentação dos Códigos}

Os códigos expostos no Capítulo 4 foram esclarecidos de maneira clara e direta, visando tornar eles compreensíveis para todos os leitores, independentemente do grau de habilidade técnica. A estratégia selecionada tem como objetivo assegurar que até mesmo aqueles sem formação em Tecnologia da Informação (TI) possam entender os princípios das implementações feitas.

Os leitores que desejam uma explicação mais minuciosa ou um nível técnico mais avançado podem encontrar os códigos com explicação completa no repositório do projeto no GitHub. O repositório pode ser acessado através do link a seguir: \href{https://github.com/sabrinaberno/TCC1}{GitHub do TCC}.