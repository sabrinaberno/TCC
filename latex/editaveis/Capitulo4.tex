\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green!50!black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    showstringspaces=false,
    breaklines=true,
    frame=single,
    captionpos=b,
    tabsize=4,
    literate={á}{{\'a}}1 {â}{{\^a}}1 {ã}{{\~a}}1 {é}{{\'e}}1 {ê}{{\^e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ô}{{\^o}}1 {õ}{{\~o}}1 {ú}{{\'u}}1 {ç}{{\c{c}}}1
}

\chapter[Execução da Pesquisa]{Execução da Pesquisa}

Antes de começar a "Execução da Pesquisa", é crucial enfatizar que todas as fases deste estudo foram realizadas de acordo com as normas éticas e legais pertinentes à investigação científica. A documentação que comprova isso está devidamente incluída nos anexos deste estudo, incluindo os seguintes documentos essenciais:
\begin{itemize}
  \item \textbf{Parecer da Plataforma Brasil}: Inclui a permissão ética que valida a utilização dos dados dos embriões para a execução deste estudo, garantindo a conformidade com os padrões éticos definidos para pesquisas que envolvem seres humanos (disponível no Anexo \ref{anexo:parecer-cep}) 
  \item \textbf{Contrato de Autorização para Utilização de Dados em Pesquisa}: Assinado pelo Dr. Bruno Ramalho, que autoriza o uso dos dados clínicos de suas pacientes, essenciais para a modelagem e análise realizadas durante esta pesquisa (disponível no Anexo \ref{anexo:contrato-autorizacao})
  \item \textbf{Termo de Consentimento para Utilização de Dados de Entrevistas, Gravação de Reuniões e Uso de Gravação}: Contrato que permite o uso de discursos, dados e gravações provenientes das reuniões e entrevistas conduzidas com o Dr. Bruno Ramalho, garantindo que as informações e contribuições fornecidas por ele fossem utilizadas com total autorização (disponível no Anexo \ref{anexo:termo-consentimento}). 
\end{itemize}
Estes documentos evidenciam o comprometimento ético e a transparência desta pesquisa, enfatizando que todas as informações empregadas foram obtidas de maneira responsável e autorizada. 

\section{Fase 1: Análise e Preparação de Dados}
\subsection{OE1 - Expansão, Processamento e Análise de Dados para Predição de Ploidia}
\subsubsection{Atividade 1 (A1): Análise, Revisão, Seleção e Limpeza de Variáveis para Predição de Euploidia}
O desenvolvimento dessa atividade teve início com a elaboração do referencial teórico, abordando os conceitos fundamentais relacionados ao aprendizado supervisionado e às tecnologias aplicadas no contexto da fertilização in vitro, além de nos aprofundarmos em pesquisas relacionadas a área da medicina reprodutiva. A pesquisa teórica foi direcionada ao entendimento dos padrões morfocinéticos de embriões capturados por sistemas de Time-Lapse e de como essas informações podem ser utilizadas para prever a porcentagem de euploidia, um indicador crítico da saúde genética do embrião.

Os dados utilizados neste estudo foram obtidos por meio do especialista, \citeonline{ramalho2024}, e de suas pacientes, que autorizaram o uso dessas informações para fins de pesquisa, contratos esses que estão na sessão de Anexo. As informações foram coletadas na clínica Bruno Ramalho Reprodução Humana, bem como na clínica Genesis, onde o \citeonline{ramalho2024} também atua.

A primeira etapa dessa atividade consiste em verificar a pertinência das variáveis já existentes na \textbf{“Planilha Original de Dados dos Embriões”}, que se encontra no Anexo \ref{anexo:planilha-original}, analisando seu impacto no desempenho do modelo preditivo. Essa análise também busca identificar se há necessidade de introduzir novas variáveis que possam melhorar a precisão do modelo ou excluir aquelas que se mostram irrelevantes.


\subsubsection{Limpeza dos Dados}
Ao lidar com os dados ausentes, utilizamos o método de Análise de Casos Completos (ACC), que envolve a remoção de observações que possuem pelo menos um valor ausente \cite{camargos2011}. Este procedimento é frequentemente empregado quando o número de dados ausentes é reduzido, assegurando que a remoção de algumas observações não interfira de forma significativa na análise estatística e preserva a consistência do modelo \cite{camargos2011}. Em nosso cenário, das 84 linhas de dados disponíveis, apenas 2 apresentavam valores ausentes. As células sem dados estão em preto na \textbf{"Planilha com Indicação de Dados Ausentes"} no Anexo \ref{anexo:planilha-dados-ausentes}. 

\subsubsection{Atividade 2 (A2): Identificação da Correlação entre os Parâmetros na Previsão da Ploidia do Embrião}

O coeficiente de \textbf{correlação de Spearman}, explicado no APÊNDICE \ref{apendice:spearman}, foi escolhido para esta atividade devido à necessidade de reconhecer relações entre os parâmetros documentados nos dados dos embriões e a porcentagem de euploidia. A metodologia leva em conta a classificação ordinal das observações, minimizando os efeitos de valores atípicos e permitindo a análise de variáveis com distribuições não normais \cite{sousa2019}. Além disso, a sua facilidade de uso em pesquisas científicas e a sua vasta aplicação em estudos destacam sua utilidade na análise de ploidia dos embriões.

O programa em Python criado para essa análise foi desenvolvido para ser modular, eficaz e gerar resultados claros tanto em formato visual quanto textual. Ele emprega as bibliotecas \textit{Pandas, SciPy, Matplotlib e python-docx} para manipular dados, determinar correlações, produzir gráficos e elaborar relatórios em Word. A seguir, detalha-se a lógica do código:

\begin{itemize}
    \item \textbf{Carregamento e Preparação dos Dados}: As informações dos embriões foram processadas a partir da Planilha de Dados Refinada, encontrada no Anexo \ref{anexo:planilha-refinada}, utilizando todas as colunas disponíveis. Essa etapa garante que todas as variáveis relevantes sejam consideradas.
    \item \textbf{Cálculo da Correlação de Spearman}: O coeficiente de Spearman foi calculado para todas as combinações possíveis de variáveis, utilizando a função \texttt{spearmanr} da biblioteca SciPy. O método \texttt{"omit"} foi empregado para lidar com valores ausentes, garantindo maior robustez mesmo que não existam dados faltantes nesta planilha.
    \item \textbf{Criação de Gráficos de Dispersão}: Para cada par de variáveis, foram criados gráficos de dispersão que auxiliam na análise numérica e permitem a identificação visual de padrões. As cores foram padronizadas, com a variável 1 (\texttt{var1}) em verde claro e marcador ``o'', e a variável 2 (\texttt{var2}) em azul escuro e marcador ``x''.
    \item \textbf{Elaboração de Relatório Automatizado}: O relatório gerado contém descrições textuais dos coeficientes e gráficos correspondentes. Este documento automatizado facilita a comunicação visual e escrita dos resultados.
\end{itemize}

O código em Python utilizado para realizar a análise está apresentado a seguir:

\begin{lstlisting}
  import pandas as pd
  from scipy.stats import spearmanr
  import matplotlib.pyplot as plt
  from docx import Document
  from docx.shared import Inches
  import os
  
  file_path = "PlanilhaDadosRefinados.xlsx"
  data = pd.read_excel(file_path)
  
  numerical_columns = [ 
      "Idade", "Estágio", "Morfo", "KIDScore", "t2", "t3", "t4", "t5", "t8", "tSC", "tSB", "tB", "cc2 (t3-t2)", 
      "cc3 (t5-t3)", "t5-t2", "s2 (t4-t3)", "s3 (t8-t5)", "tSC-t8", "tB-tSB", "Ploidia"
  ]
  
  numerical_columns = [col for col in numerical_columns if col in data.columns]
  
  results = []
  for col1 in numerical_columns:
      for col2 in numerical_columns:
          if col1 != col2:  
              coef, _ = spearmanr(data[col1], data[col2], nan_policy="omit")
              results.append({"Variable 1": col1, "Variable 2": col2, "Spearman Coefficient": coef})
  
  correlation_df = pd.DataFrame(results)
  
  output_file = "correlation_results.xlsx"
  correlation_df.to_excel(output_file, index=False)
  print(f"\nResultados salvos no arquivo: {output_file}")
  
  doc = Document()
  doc.add_heading('Correlação de Spearman - Embriões', 0)
  
  output_dir = "gráficos"
  os.makedirs(output_dir, exist_ok=True)
  
  color_var1 = "lightgreen"
  color_var2 = "darkblue"
  
  for index, row in correlation_df.iterrows():
      var1, var2 = row['Variable 1'], row['Variable 2']
      coef = row['Spearman Coefficient']
  
      plt.figure(figsize=(6, 4))
      plt.scatter(data[var1], data[var2], alpha=0.6, c=color_var1, label=f'{var1}', marker='o')
      plt.scatter(data[var2], data[var1], alpha=0.6, c=color_var2, label=f'{var2}', marker='x')
      
      plt.title(f"Dispersão entre {var1} e {var2} (Coeficiente de Spearman: {coef:.2f})")
      plt.xlabel(var1)
      plt.ylabel(var2)
  
      img_path = os.path.join(output_dir, f"grafico_{var1}_{var2}.png")
      plt.savefig(img_path)
      plt.close()
  
      doc.add_heading(f'Correlação entre {var1} e {var2}', level=1)
      doc.add_paragraph(f'Coeficiente de Spearman: {coef:.2f}')
      doc.add_picture(img_path, width=Inches(5.0))
  
  output_word = "relatorio_correlacoes.docx"
  doc.save(output_word)
  
  print(f"\nRelatório gerado e salvo em: {output_word}")
\end{lstlisting}

O código analisa a correlação entre as variáveis numéricas em uma sequência de dados, empregando o coeficiente de Spearman. Ele inicia carregando a planilha Excel com as informações dos embriões e seleciona todas as colunas relevantes para a análise. O coeficiente de Spearman é calculado para cada par de variáveis, avaliando a intensidade e a direção da relação monótona entre elas. Os resultados são armazenados em um \texttt{DataFrame}, que é uma estrutura de dados bidimensional do Pandas, semelhante a uma tabela, permitindo fácil manipulação e análise. Esta tabela é então exportada para um arquivo Excel chamado \texttt{correlation\_results.xlsx}.

Após isso, o programa gera gráficos de dispersão para observar as correlações, ressaltando os padrões das variáveis associadas. Esses gráficos, juntamente com os coeficientes de correlação, são automaticamente incorporados a um documento Word. No final, temos um relatório completo com as análises e visualizações, salvo como um arquivo chamado \texttt{relatorio\_correlacoes.docx}.

\subsubsection{Atividade 3 (A3): Normalização dos Dados para Otimização}
A etapa de normalização dos dados é um passo fundamental na criação de modelos de aprendizado de máquina, particularmente em situações onde as variáveis têm diferentes escalas e distribuições. Esta tarefa foi executada com o uso do método Z-Score. Esta metodologia modifica os dados de forma que cada variável possua uma média de 0 e um desvio padrão de 1, explicado na APÊNDICE \ref{apendice:zscore}. 

A normalização foi conduzida com Python e a biblioteca Pandas. O procedimento foi automatizado para simplificar a análise em grande escala dos dados morfocinéticos dos embriões. A normalização foi aplicada especificamente às variáveis numéricas do conjunto de dados, previamente selecionadas na fase de escolha das variáveis. O código utilizado para realizar essa atividade foi:

\begin{lstlisting}
  import pandas as pd
  import numpy as np
  
  
  file_path = "PlanilhaDeDadosDosEmbriões4.xlsx"  
  data = pd.read_excel(file_path)
  
  
  numeric_columns = data.select_dtypes(include=[np.number]).columns
  
  
  normalized_data = data.copy()
  
  
  for col in numeric_columns:
      mean = data[col].mean()
      std = data[col].std()
      normalized_data[col] = (data[col] - mean) / std
  
  
  output_file = "Planilha_normalizada.xlsx"
  normalized_data.to_excel(output_file, index=False)
  
  
  print(f"Normalização concluída. Arquivo salvo como: {output_file}")
\end{lstlisting}  

A sequência de passos executados pelo código para a normalização dos dados foi:

\begin{enumerate}
    \item \textbf{Análise dos Dados:} Os dados foram inicialmente importados do arquivo Excel que possui os dados dos embriões, usando a função \texttt{pd.read\_excel()}.
    
    \item \textbf{Identificação das Colunas Numéricas:} Em seguida, foram identificadas as colunas numéricas do \textit{DataFrame} através do método \texttt{select\_dtypes()}. Essa etapa é muito importante, já que a normalização com Z-Score deve ser realizada apenas em variáveis contínuas. As variáveis normalizadas foram: Idade, Kidscore, t2, t3, t4, t5, t8, tSC, tSB, tB, cc2 (\texttt{t3-t2}), cc3 (\texttt{t5-t3}), \texttt{t5-t2}, s2 (\texttt{t4-t3}), s3 (\texttt{t8-t5}), \texttt{tSC-t8} e \texttt{tB-tSB}.
    
    \item \textbf{Cálculo da Média e Desvio Padrão:} Para cada variável numérica, a média e o desvio padrão foram calculados. Esses valores são fundamentais para a utilização da fórmula do Z-Score, apresentada e explicada no \textbf{APÊNDICE \ref{apendice:zscore}}.
    
    \item \textbf{Utilização do Z-Score:} Para cada valor de uma variável, a média é subtraída e o desvio padrão é dividido, de acordo com a fórmula. Isso modifica os dados para que todas as variáveis apresentem média zero e desvio padrão igual a 1.
    
    \item \textbf{Armazenamento dos Dados Normalizados:} Depois de aplicar o Z-Score, os dados normalizados foram guardados em uma nova planilha Excel, denominada “\texttt{Planilha\_normalizada.xlsx}”. A planilha está em “\texttt{Atividade 3}” no GitHub do nosso projeto: \textbf{\href{https://github.com/sabrinaberno/TCC1}{GitHub-TCC}}.
\end{enumerate}

A opção pelo Z-Score se deve à sua habilidade de converter os dados para uma escala padrão, preservando as informações pertinentes. Pesquisas apontam que a normalização melhora a precisão dos algoritmos de aprendizado de máquina ao remover os efeitos de escalas distintas entre as variáveis \cite{jaiswal2024}. Além disso, essa técnica simplifica a comparação entre variáveis, auxiliando na interpretação e validação dos modelos criados.

\subsubsection{Atividade 4 (A4): Separar o conjunto de dados em conjuntos de treinamento, validação e teste, fazendo uma distribuição dos dados e aplicar técnica de aumento de dados}
Nessa atividade, apresentamos a implementação do procedimento de segmentação do conjunto de dados em três subconjuntos diferentes: treinamento, validação e teste. Esta distribuição obedece às proporções estabelecidas na metodologia (70\%, 15\% e 15\%, respectivamente) e foi feita através de um \textit{script} em Python. Utilizamos a planilha criada pela Atividade 3, a ``\texttt{Planilha\_normalizada.xlsx}'', pois utilizaremos os dados já normalizados e tratados a partir de agora.

A segmentação do conjunto de dados foi planejada para assegurar que cada subconjunto tenha uma representação adequada para seus respectivos propósitos, com já citado na Capítulo 3 em Metodologia. A distribuição foi feita de maneira aleatória para prevenir viés e assegurar a aplicabilidade geral do modelo. O \textit{script} calculou o tamanho de cada subconjunto com base em um total 82 linhas de amostra, seguindo as porcentagens estabelecidas.

Abaixo, descrevemos o código implementado, seguido de sua explicação detalhada:

\begin{lstlisting}
  import pandas as pd
  import numpy as np
  
  
  df = pd.read_excel("Planilha_normalizada.xlsx")
  
  
  df = df.sample(frac=1, random_state=42).reset_index(drop=True)
  
  
  n_total = len(df)
  n_train = int(n_total * 0.7)
  n_val = int(n_total * 0.15)
  n_test = n_total - n_train - n_val
  
  
  train_data = df.iloc[:n_train]
  val_data = df.iloc[n_train:n_train + n_val]
  test_data = df.iloc[n_train + n_val:]
  
  
  train_data.to_excel("dados_treinamento.xlsx", index=False)
  val_data.to_excel("dados_validacao.xlsx", index=False)
  test_data.to_excel("dados_teste.xlsx", index=False)
  
  
  print(f"Conjunto de Treinamento: {len(train_data)} linhas")
  print(f"Conjunto de Validação: {len(val_data)} linhas")
  print(f"Conjunto de Teste: {len(test_data)} linhas")
\end{lstlisting}  

\begin{itemize}
  \item \textbf{Importação das Bibliotecas:} A biblioteca \texttt{pandas} foi empregada para a manipulação de dados e \texttt{numpy} para realizar operações matemáticas.

  \item \textbf{Carregamento dos Dados:} O documento Excel que continha a base de dados dos embriões foi carregado em um \textit{DataFrame} do \texttt{pandas} para simplificar as operações subsequentes.

  \item \textbf{Embaralhamento dos Dados:} Utilizamos a função \texttt{sample} com os parâmetros \texttt{(frac=1 e random\_state=42)} para embaralhar os dados de maneira aleatória, assegurando a remoção de qualquer padrão sequencial existente no conjunto original. O argumento \texttt{random\_state} garante a reprodutibilidade, possibilitando que o mesmo resultado seja alcançado em futuras execuções.

  \item \textbf{Cálculo das Proporções:} As proporções foram estabelecidas para os conjuntos de treinamento, validação e teste, assegurando que a soma das dimensões corresponda ao número total de amostras. Isso é crucial para prevenir perdas de informações ou desbalanceamento nos conjuntos.

  \item \textbf{Divisão dos Dados:} A escolha dos subconjuntos foi feita com base nos índices do \textit{DataFrame}, que foram divididos em intervalos definidos com base nos tamanhos previamente calculados.

  \item \textbf{Armazenamento dos Dados:} Os subconjuntos resultantes foram salvos em arquivos Excel separados utilizando a função \texttt{to\_excel}, com o parâmetro \texttt{index=False} para evitar a inclusão de índices desnecessários nos arquivos finais. Os arquivos foram nomeados para refletir suas respectivas funções: \texttt{dados\_treinamento.xlsx}, \texttt{dados\_validacao.xlsx} e \texttt{dados\_teste.xlsx}. 
\end{itemize}

O script priorizou a simplicidade, a replicabilidade e a eficácia. Através deste método, garantimos que os conjuntos produzidos sejam apropriados para as fases seguintes de treinamento, validação e teste do modelo de IA, mantendo a integridade dos dados originais.

\subsection*{Aumento de Dados (\textit{Data Augmentation}) utilizando o Método de Monte Carlo:} 
A técnica de aumento de dados (\textit{data augmentation}) foi utilizada para melhorar o desempenho do modelo ao gerar mais exemplos de treinamento a partir dos dados existentes. Este processo visa aumentar a capacidade de generalização do modelo e evitar o \textit{overfitting}, tornando-o mais robusto. Abaixo, descrevemos o código implementado, seguido de sua explicação detalhada:

\begin{lstlisting}
  import pandas as pd
  import numpy as np
  
  
  train_data_path = 'dados_treinamento.xlsx'  
  df_train = pd.read_excel(train_data_path)
  
  
  numerical_columns = df_train.select_dtypes(include=['float64', 'int64']).columns
  categorical_columns = df_train.select_dtypes(include=['object']).columns
  
  
  augmentation_factor = 3  
  num_new_samples = len(df_train) * augmentation_factor
  
  
  new_samples = {}
  for col in numerical_columns:
      mean = df_train[col].mean()
      std = df_train[col].std()
      new_samples[col] = np.random.normal(loc=mean, scale=std, size=num_new_samples)
  
  
  df_new_samples = pd.DataFrame(new_samples)
  
  
  for col in categorical_columns:
      replicated_values = np.random.choice(df_train[col], size=num_new_samples, replace=True)
      df_new_samples[col] = replicated_values
  
  
  df_augmented = pd.concat([df_train, df_new_samples], ignore_index=True)
  
  
  augmented_data_path = 'dados_treinamento_aumentado.xlsx'
  df_augmented.to_excel(augmented_data_path, index=False)
  
  
  print(f"Conjunto de dados aumentado salvo em: {augmented_data_path}")
\end{lstlisting}

O programa emprega distribuições estatísticas autênticas para produzir novas amostras que seguem os padrões dos dados originais. A distinção entre dados numéricos e categóricos possibilita o tratamento adequado de cada variável, garantindo a integridade do conjunto de dados ampliado.

\begin{itemize}
  \item \textbf{Importação das Bibliotecas:} As bibliotecas \texttt{pandas} e \texttt{numpy} foram utilizadas para manipular dados e realizar operações matemáticas: o \texttt{pandas} foi empregado para carregar, manipular e armazenar os conjuntos de dados, enquanto o \texttt{numpy} foi responsável pela criação de valores aleatórios para as colunas numéricas e pela replicação de valores categóricos.

  \item \textbf{Carregamento do Conjunto de Dados de Treinamento:} O documento  com os \texttt{dados de treinamento} foi inserido num \textit{DataFrame}, que atua como uma estrutura para a organização e manipulação eficaz dos dados. A variável \texttt{train\_data\_path} definiu o caminho do arquivo, enquanto a função \texttt{pd.read\_excel} foi empregada para acessar os dados.

  \item \textbf{Identificação das Colunas Numéricas e Categóricas:} Foram escolhidas colunas numéricas (como \texttt{float64} ou \texttt{int64}) para produzir novos valores com base em distribuições estatísticas, enquanto as colunas categóricas (do tipo \texttt{object}) para duplicar valores já existentes, preservando as proporções originais. Esta divisão foi feita por meio dos métodos \texttt{select\_dtypes}, que possibilitaram a identificação e categorização das colunas com base no seu tipo de informação.

  \item \textbf{Definição do Fator de Aumento:} O fator de ampliação foi estipulado em 3, o que sugere que o volume do conjunto de treinamento será triplicado. A quantidade de amostras adicionais foi determinada ao multiplicar o tamanho original do conjunto (\texttt{len(df\_train)}) pelo fator de ampliação. Isso estabeleceu o número de novos exemplos a serem produzidos para ampliar a base de dados de treinamento.

  \item \textbf{Geração de Novas Amostras para Colunas Numéricas:} Para cada coluna numérica, foram criados novos valores por meio da distribuição normal. Os parâmetros utilizados foram o valor médio (\texttt{mean}) e o desvio padrão (\texttt{std}) da coluna original, assegurando que os novos valores mantenham as propriedades estatísticas da distribuição original. Este procedimento garante que os dados expandidos para as colunas numéricas mantenham a mesma distribuição da amostra inicial.

  \item \textbf{Replicação de Valores para Colunas Categóricas:} Os valores já existentes nas colunas categóricas foram replicados de maneira aleatória por meio da função \texttt{np.random.choice}, com reposição. Esta estratégia mantém a proporção original dos valores categóricos, assegurando a uniformidade nos padrões e possibilitando a manutenção da distribuição das categorias nos novos exemplos criados.

  \item \textbf{Combinação dos Dados Originais com os Novos:} A meta era unir os dados originais aos novos dados produzidos, resultando em um conjunto de treinamento expandido. Com o parâmetro \texttt{ignore\_index=True}, a função \texttt{pd.concat} foi empregada para recomeçar os índices no novo conjunto de dados, assegurando que o \textit{DataFrame} gerado apresentasse uma sequência ininterrupta de índices, sem duplicações ou sobreposições.

  \item \textbf{Armazenamento do Conjunto de Dados Ampliado:} O arquivo Excel com os dados ampliados foi nomeado como \texttt{dados\_treinamento\_aumentado.xlsx}. O parâmetro \texttt{index=False} foi empregado para prevenir a criação de índices nos arquivos finais, facilitando sua utilização em fases subsequentes do trabalho e assegurando que o arquivo gerado contenha apenas os dados, sem colunas de índice adicionais.
\end{itemize}

A aplicação da média e do desvio padrão assegura que os novos dados numéricos correspondam às características do conjunto original, ao mesmo tempo que as categorias mantêm suas proporções, mantendo os padrões originais. A produção de dados por meio de distribuições estatísticas amplia a diversidade do conjunto sem adicionar ruído excessivo ou padrões inverossímeis, assegurando uma diversificação controlada. Este procedimento garante que o conjunto de treinamento ampliado seja robusto o suficiente para treinar o modelo, prevenindo sobrecargas e aprimorando a generalização, levando a um modelo mais eficiente e seguro.

\subsubsection{Nota sobre a Apresentação dos Códigos}

Os códigos expostos no Capítulo 4 foram esclarecidos de maneira clara e direta, visando tornar eles compreensíveis para todos os leitores, independentemente do grau de habilidade técnica. A estratégia selecionada tem como objetivo assegurar que até mesmo aqueles sem formação em Tecnologia da Informação (TI) possam entender os princípios das implementações feitas.

Os leitores que desejam uma explicação mais minuciosa ou um nível técnico mais avançado podem encontrar os códigos com explicação completa no repositório do projeto no GitHub. O repositório pode ser acessado através do link a seguir: \href{https://github.com/sabrinaberno/TCC1}{GitHub do TCC}.